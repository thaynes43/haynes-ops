{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Haynes Ops Cluster Repo","text":"<p>Welcome to the documents for my home ops style cluster repo. I've put these together so I can remember how it works but you are welcome to read them too!</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"cheat-sheets/ceph/","title":"Ceph Cheat Sheet","text":"<p>Merge of my notes from proxmox and commands from this guide</p>"},{"location":"cheat-sheets/ceph/#rook","title":"Rook","text":"<p>Restart operator to re-invoke cluster init:</p> <pre><code>kubectl -n rook-ceph delete pod -l app=rook-ceph-operator\n</code></pre> <p>Get into a place you can use the regular commands:</p> <pre><code>kubectl --namespace rook-ceph exec -it deploy/rook-ceph-operator -- bash\n</code></pre> <p>TODO Move this</p> <p><pre><code>kubectl --namespace rook-ceph exec -it deploy/rook-ceph-operator -- bash\nrook multus validation run --public-network=network/multus-public --cluster-network=network/multus-ceph -n rook-ceph\n</code></pre> This leaves a MESS so you need to run:</p> <pre><code>rook multus validation cleanup --namespace rook-ceph\n</code></pre> <p>Then the next validation:</p> <pre><code>rook multus validation config converged\n</code></pre> <pre><code>Example:\n\nthaynes@HaynesHyperion:~$ kubectl --namespace rook-ceph exec -it deploy/rook-ceph-operator -- bash\nDefaulted container \"rook-ceph-operator\" out of: rook-ceph-operator, k8tz (init)\n[rook@rook-ceph-operator-69745fc466-pc95r /]$ rook multus validation run --help\n2024/10/26 10:59:57 maxprocs: Leaving GOMAXPROCS=20: CPU quota undefined\n\nRun a validation test that determines whether the current Multus and system\nconfigurations will support Rook with Multus.\n\nThis should be run BEFORE Rook is installed.\n\nThis is a fairly long-running test. It starts up a web server and many\nclients to verify that Multus network communication works properly.\n\nIt does *not* perform any load testing. Networks that cannot support high\nvolumes of Ceph traffic may still encounter runtime issues. This may be\nparticularly noticeable with high I/O load or during OSD rebalancing\n(see: https://docs.ceph.com/en/latest/architecture/#rebalancing).\nFor example, during Rook or Ceph cluster upgrade.\n\nOverride the kube config file location by setting the KUBECONFIG environment variable.\n\nUsage:\n  rook multus validation run [--public-network=&lt;nad-name&gt;] [--cluster-network=&lt;nad-name&gt;] [flags]\n\nFlags:\n      --cluster-network string                   The name of the Network Attachment Definition (NAD) that will be used for Ceph's cluster network. This should be a namespaced name in the form &lt;namespace&gt;/&lt;name&gt; if the NAD is defined in a different namespace from the cluster namespace.\n  -c, --config string                            The validation test config file to use. This cannot be used with other flags except --host-check-only.\n      --daemons-per-node int                     The number of validation test daemons to run per node. It is recommended to set this to the maximum number of Ceph daemons that can run on any node in the worst case of node failure(s). The default value is set to the worst-case value for a Rook Ceph cluster with 3 portable OSDs, 3 portable monitors, and where all optional child resources have been created with 1 daemon such that they all might run on a single node in a failure scenario. If you aren't sure what to choose for this value, add 1 for each additional OSD beyond 3. (default 19)\n      --flaky-threshold-seconds timeoutSeconds   This is the time window in which validation clients are all expected to become 'Ready' together. Validation clients are all started at approximately the same time, and they should all stabilize at approximately the same time. Once the first validation client becomes 'Ready', the tool checks that all of the remaining clients become 'Ready' before this threshold duration elapses. In networks that have connectivity issues, limited bandwidth, or high latency, clients will contend for network traffic with each other, causing some clients to randomly fail and become 'Ready' later than others. These randomly-failing clients are considered 'flaky.' Adjust this value to reflect expectations for the underlying network. For fast and reliable networks, this can be set to a smaller value. For networks that are intended to be slow, this can be set to a larger value. Additionally, for very large Kubernetes clusters, it may take longer for all clients to start, and it therefore may take longer for all clients to become 'Ready'; in that case, this value can be set slightly higher. (default 30s)\n  -h, --help                                     help for run\n      --host-check-only                          Only check that hosts can connect to the server via the public network. Do not start clients. This mode is recommended when a Rook cluster is already running and consuming the public network specified.\n  -n, --namespace string                         The namespace for validation test resources. It is recommended to set this to the namespace in which Rook's Ceph cluster will be installed. (default \"rook-ceph\")\n      --nginx-image string                       The Nginx image used for the validation server and clients. (default \"quay.io/nginx/nginx-unprivileged:stable-alpine\")\n      --public-network string                    The name of the Network Attachment Definition (NAD) that will be used for Ceph's public network. This should be a namespaced name in the form &lt;namespace&gt;/&lt;name&gt; if the NAD is defined in a different namespace from the cluster namespace.\n      --service-account string                   The name of the service account that will be used for test resources. (default \"rook-ceph-system\")\n      --timeout-minutes timeoutMinutes           The time to wait for resources to change to the expected state. For example, for the test web server to start, for test clients to become ready, or for test resources to be deleted. At longest, this may need to reflect the time it takes for client pods to to pull images, get address assignments, and then for each client to determine that its network connection is stable. Minimum: 1 minute. Recommended: 2 minutes or more. (default 3m0s)\n\nGlobal Flags:\n      --log-level string   logging level for logging/tracing output (valid values: ERROR,WARNING,INFO,DEBUG) (default \"INFO\")\n[rook@rook-ceph-operator-69745fc466-pc95r /]$ rook multus validation config --help\n2024/10/26 11:00:16 maxprocs: Leaving GOMAXPROCS=20: CPU quota undefined\nGenerate a validation test config file for different default scenarios to stdout.\n\nUsage:\n  rook multus validation config [command]\n\nAvailable Commands:\n  converged               Example config for a cluster that runs storage and user workloads on all nodes.\n  dedicated-storage-nodes Example config file for a cluster that uses dedicated storage nodes.\n  stretch-cluster         Example config file for a stretch cluster with dedicated storage nodes.\n\nFlags:\n  -h, --help   help for config\n\nGlobal Flags:\n      --log-level string   logging level for logging/tracing output (valid values: ERROR,WARNING,INFO,DEBUG) (default \"INFO\")\n\nUse \"rook multus validation config [command] --help\" for more information about a command.\n</code></pre>"},{"location":"cheat-sheets/ceph/#talos","title":"Talos","text":""},{"location":"cheat-sheets/ceph/#status","title":"Status","text":"<p>We can use a tools pod to run <code>ceph</code> commands</p>"},{"location":"cheat-sheets/ceph/#check-status","title":"Check Status","text":"<pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status\n</code></pre>"},{"location":"cheat-sheets/ceph/#clear-warn","title":"Clear Warn:","text":"<pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status\n</code></pre>"},{"location":"cheat-sheets/ceph/#misc-tood","title":"Misc TOOD","text":"<pre><code>kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=0\nkubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=1\n</code></pre> <pre><code>kubectl get storageclass\n\nkubectl -n rook-ceph get cephclusters rook-ceph\n\nkubectl -n rook-ceph get cephclusters\n</code></pre> <p>Pre-condtion before talos upgrade:</p> <pre><code>kubectl -n rook-ceph wait --timeout=1800s --for=jsonpath='{.status.ceph.health}=HEALTH_OK' rook-ceph\n</code></pre>"},{"location":"cheat-sheets/ceph/#remove-from-k8s","title":"Remove from k8s","text":"<pre><code>kubectl -n rook-ceph patch cephcluster rook-ceph --type merge -p '{\"spec\":{\"cleanupPolicy\":{\"confirmation\":\"yes-really-destroy-data\"}}}'\n\nkubectl delete storageclasses ceph-block ceph-bucket ceph-filesystem\n\nkubectl -n rook-ceph delete cephblockpools ceph-blockpool\n\nkubectl -n rook-ceph delete cephobjectstore ceph-objectstore\n\nkubectl -n rook-ceph delete cephfilesystem ceph-filesystem\n</code></pre> <p>Now delete cluster:</p> <pre><code>kubectl -n rook-ceph delete cephcluster rook-ceph\n\nhelm -n rook-ceph uninstall rook-ceph-cluster\n</code></pre> <p>Now the operator: <pre><code>helm -n rook-ceph uninstall rook-ceph\n</code></pre></p>"},{"location":"cheat-sheets/ceph/#cluseter-finalizer","title":"Cluseter Finalizer","text":"<pre><code>kubectl patch cephcluster rook-ceph -n rook-ceph --type=json -p='[{\"op\": \"remove\", \"path\": \"/metadata/finalizers\"}]'\n</code></pre> <pre><code>for CRD in $(kubectl get crd -n rook-ceph | awk '/ceph.rook.io/ {print $1}'); do\n    kubectl get -n rook-ceph \"$CRD\" -o name | \\\n    xargs -I {} kubectl patch -n rook-ceph {} --type merge -p '{\"metadata\":{\"finalizers\": []}}'\ndone\n</code></pre> <pre><code>kubectl -n rook-ceph patch configmap rook-ceph-mon-endpoints --type merge -p '{\"metadata\":{\"finalizers\": []}}'\nkubectl -n rook-ceph patch secrets rook-ceph-mon --type merge -p '{\"metadata\":{\"finalizers\": []}}'\n</code></pre>"},{"location":"cheat-sheets/ceph/#finish-off-metadata","title":"Finish off metadata","text":"<p>Talos may need some massaging:</p> <pre><code>$ cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: disk-clean\nspec:\n  restartPolicy: Never\n  nodeName: &lt;storage-node-name&gt;\n  volumes:\n  - name: rook-data-dir\n    hostPath:\n      path: &lt;dataDirHostPath&gt;\n  containers:\n  - name: disk-clean\n    image: busybox\n    securityContext:\n      privileged: true\n    volumeMounts:\n    - name: rook-data-dir\n      mountPath: /node/rook-data\n    command: [\"/bin/sh\", \"-c\", \"rm -rf /node/rook-data/*\"]\nEOF\npod/disk-clean created\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-clean\npod/disk-clean condition met\n\n$ kubectl delete pod disk-clean\npod \"disk-clean\" deleted\n</code></pre> <p>And wipe the disks:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: disk-wipe\n  namespace: rook-ceph\nspec:\n  restartPolicy: Never\n  nodeName: talosm03\n  containers:\n  - name: disk-wipe\n    image: busybox\n    securityContext:\n      privileged: true\n    command: [\"/bin/sh\", \"-c\", \"dd if=/dev/zero bs=1M count=100 oflag=direct of=/dev/nvme1n1\"]\nEOF\npod/disk-wipe created\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-wipe\npod/disk-wipe condition met\n\n$ kubectl delete pod disk-wipe\npod \"disk-wipe\" deleted\n</code></pre>"},{"location":"cheat-sheets/ceph/#ceph","title":"Ceph","text":"<p>Not sure how these would work unless you install ceph and a config that points to the cluster on another machine (seeing you can't run ceph cli on talos)</p>"},{"location":"cheat-sheets/ceph/#archive-crash-warnings","title":"Archive crash warnings","text":"<p>These happen when I reboot.</p> <pre><code>ceph crash archive-all\n</code></pre>"},{"location":"cheat-sheets/ceph/#cluster-info","title":"Cluster info","text":""},{"location":"cheat-sheets/ceph/#status_1","title":"Status","text":"<p>Overall status of the cluster.</p> <pre><code>ceph status || ceph -w\n</code></pre>"},{"location":"cheat-sheets/ceph/#config","title":"Config","text":"<pre><code>ceph config dump\n</code></pre>"},{"location":"cheat-sheets/ceph/#monitors","title":"Monitors","text":"<p>Get details about the monitors.</p> <pre><code>ceph mon dump\n</code></pre>"},{"location":"cheat-sheets/ceph/#ceph-services","title":"Ceph Services","text":""},{"location":"cheat-sheets/ceph/#see-services","title":"See Services","text":"<pre><code>ceph mgr services\n</code></pre>"},{"location":"cheat-sheets/ceph/#restart-service","title":"Restart Service","text":"<pre><code>ceph mgr module disable dashboard\nceph mgr module enable dashboard\n</code></pre>"},{"location":"cheat-sheets/kubernetes/","title":"Kubernetes Cheat Sheet","text":"<p>Find api resources for namespace</p> <pre><code>kubectl api-resources --verbs=list --namespaced -o name \\\n  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n &lt;namespace&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#flux","title":"Flux","text":"<pre><code>kubectl logs -n flux-system deploy/helm-controller\n</code></pre> <p>title: Kubernetes Cheat Sheet permalink: /cheatsheets/kube-commands/</p>"},{"location":"cheat-sheets/kubernetes/#retries-exceeded","title":"Retries Exceeded","text":"<p>I see this when things take a while because something else is broken (like a csi) but there is no reason to fix the thing that is stuck. First force helmrelease to get fixed:</p> <pre><code>flux reconcile helmrelease -n home-automation home-assistant --force\n</code></pre> <p>Then get the kustomization in a good state again (if the helmrelease was OK):</p> <pre><code>flux reconcile kustomization -n flux-system zigbee2mqtt\n</code></pre> <p>Here are some commands for managing Kubernetes.</p>"},{"location":"cheat-sheets/kubernetes/#clean-mess","title":"Clean Mess","text":"<p>Check what you are deleting:</p> <pre><code>kubectl get pod --field-selector=status.phase==Succeeded -A\nkubectl get pod --field-selector=status.phase==Failed -A\n</code></pre> <p>If nothing to debug go ahead and kill em:</p> <pre><code>kubectl delete pod --field-selector=status.phase==Succeeded -A\nkubectl delete pod --field-selector=status.phase==Failed -A\n</code></pre> <p>Rebooting sometimes creates a mess and this will clean it but I thing they go away after a bit.</p>"},{"location":"cheat-sheets/kubernetes/#dns","title":"DNS","text":"<p>DNS Test:</p> <pre><code>kubectl -n ai run dns-test --rm -it --image=busybox --restart=Never -- nslookup volsync-hayesops.s3.amazonaws.com\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectx-kubens","title":"kubectx &amp; kubens","text":"<p>These are helper scripts that come from here.</p>"},{"location":"cheat-sheets/kubernetes/#show-available-contexts-clusters","title":"Show available contexts (clusters)","text":"<pre><code>kubectx\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#switch-cluster","title":"Switch cluster","text":"<pre><code>kubectx &lt;cluster name&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#show-available-namespaces","title":"Show available namespaces","text":"<pre><code>kubens\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#switch-namespaces","title":"Switch namespaces","text":"<pre><code>kubens &lt;namespace&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectl","title":"kubectl","text":"<p>This is the beast that drives operations around your k8s cluster. Most of the heavy lifting seems to be done in the manifest yaml or helm charts but debugging and/or setting up requires heavy use of <code>kubectl</code>.</p>"},{"location":"cheat-sheets/kubernetes/#kubectl-apply","title":"kubectl apply","text":"<pre><code>kubectl apply -f &lt;filename.yaml&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectl-get","title":"kubectl get","text":"<p>To see what's running, installed, or anything else we use <code>kubectl get &lt;thing&gt;</code></p>"},{"location":"cheat-sheets/kubernetes/#get-nodes","title":"get nodes","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-namespaces","title":"get namespaces","text":"<pre><code>kubectl get namespaces\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-pods","title":"get pods","text":"<pre><code>kubectl get pods --all-namespaces\nkubectl -n &lt;namespace&gt; kubget pods \n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-services","title":"get services","text":"<p>This is useful for figuring port mappings.</p> <pre><code>kubectl get svc --all-namespaces\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-volumes","title":"get volumes","text":"<pre><code>kubectl get pv,pvc -o wide\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#browse-volume","title":"Browse Volume","text":"<pre><code>kubectl browse-pvc -n home-automation zigbee2mqtt\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-volumesnapshots","title":"get volumesnapshots","text":"<pre><code>kubectl get volumesnapshot\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-volumesnapshotclass","title":"get volumesnapshotclass","text":"<p>This will show you what type of snapshots you can take.</p> <pre><code>kubectl get volumesnapshotclass\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-ceph-cluster","title":"get ceph cluster","text":"<p>Since I used rook this tells me how the ceph cluster exposed to k8s is doing.</p> <pre><code>kubectl -n rook-ceph  get CephCluster\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectl-delete","title":"kubectl delete","text":""},{"location":"cheat-sheets/kubernetes/#delete-manifest","title":"delete manifest","text":"<pre><code>kubectl delete -f &lt;manifest.yaml&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#delete-all-your-volumes-dont-do-this","title":"delete all your volumes (don't do this!)","text":"<pre><code>kubectl delete pvc --all\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#delete-pv","title":"delete pv","text":"<pre><code>kubectl patch pvc -n namespace PVCNAME -p '{\"metadata\": {\"finalizers\": null}}'\nkubectl patch pv PVNAME -p '{\"metadata\": {\"finalizers\": null}}'\n\nk delete pvc PVCNAME --grace-period=0 --force\nk delete pv PVNAME --grace-period=0 --force\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectl-describe","title":"kubectl describe","text":"<p>Describe things in the cluster like pods and pvc's. This will give you some basic info to troubleshoot with before going to the logs (or if the logs are gone because the thing is crash looping.</p> <pre><code>kubectl describe &lt;type&gt; &lt;name&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectl-logs","title":"kubectl logs","text":"<p>For an app:</p> <pre><code>kubectl logs -n velero -l app.kubernetes.io/name=velero\n</code></pre> <p>For a pod:</p> <pre><code>kubectl logs &lt;podname&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectl-exec","title":"kubectl exec","text":"<p>Like bashing into a docker container but for a pod. Note if you are not kubens'd into the namespace you will need <code>-n &lt;namespace&gt;</code></p> <pre><code>kubectl exec -it -n &lt;namespace&gt; &lt;pod_name&gt; -- env\n</code></pre> <p>Bash in:</p> <pre><code>k exec -it -n &lt;namespace&gt;  &lt;pod_name&gt; -- sh\nk exec -it  -n &lt;namespace&gt;  &lt;pod_name&gt; -- /bin/bash\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#restart","title":"Restart","text":"<pre><code>https://kubernetes.io/docs/reference/kubectl/generated/kubectl_rollout/kubectl_rollout_restart/\n\nk rollout restart deployment/prowlarr -n media-management\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#see-what-capabilities-you-have","title":"See what capabilities you have","text":"<p>This will show resources. Here we are checking to see if the cluster can make volumesnapshots or at least is eligible to be setup for volumesnapshots.</p> <pre><code>kubectl api-resources | grep volumesnapshots\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#velero","title":"Velero","text":""},{"location":"cheat-sheets/kubernetes/#create-backup","title":"Create Backup","text":"<pre><code>velero backup create hellothere --include-namespaces=default --wait\n</code></pre> <pre><code> velero backup create --from-schedule=velero-daily-backups\n ```\n\n#### TODO FROM DEBUGGING NEEDS ORGANIZING\n\n```bash\nkubectl get events -A\nceph -s\nceph osd lspools\nkubectl logs -f velero-764d58dfd9-k47sh\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#mass-delete","title":"Mass Delete","text":""},{"location":"cheat-sheets/kubernetes/#namespace-w-spec-finalizer","title":"Namespace w/ Spec Finalizer","text":"<p>This was super hard to get rid of vs. a regular namespace! </p> <pre><code>kubectl get ns rook-ceph -o json | jq '.spec.finalizers = []' | kubectl replace --raw \"/api/v1/namespaces/rook-ceph/finalize\" -f -\n</code></pre> <pre><code>kubectl get namespace rook-ceph -o json &gt; tmp.json\n</code></pre> <ul> <li>Delete kubernetes finalizer in tmp.json (leave empty array \"finalizers\": [])</li> <li>Run kubectl proxy in another terminal for auth purposes and run following curl request to returned port</li> </ul> <pre><code>kubectl proxy &amp;\ncurl -k -H \"Content-Type: application/json\" -X PUT --data-binary @tmp.json 127.0.0.1:8001/api/v1/namespaces/rook-ceph/finalize\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#jobs","title":"Jobs","text":"<pre><code>kubectl get pod  -n velero --field-selector=status.phase==Succeeded\n\nkubectl delete pod -n velero  --field-selector=status.phase==Succeeded\n\nkubectl delete pod -n velero  --field-selector=status.phase==Failed\n</code></pre> <pre><code>kubectl get jobs  -n velero --field-selector status.successful=1\n\nkubectl delete jobs -n velero --field-selector status.successful=1\n</code></pre> <p>Pass <code>`--all-namespaces</code> for a good time!</p> <pre><code>velero delete backup move-data-test -n velero\nvelero delete backup velero-daily-backups-20240827000047 -n velero\nvelero delete backup velero-daily-backups-20240826000046 -n velero\n</code></pre> <p>TANK CHECK:</p> <pre><code>velero describe backup move-data-test-smb --details\n</code></pre> <pre><code>kubectl -n velero rollout restart daemonset node-agent\nkubectl -n velero rollout restart deployment velero\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#stuck-stuff","title":"Stuck stuff","text":"<pre><code>kubectl get volumeattachment\n</code></pre> <p>See if it's attached, remove finalizes:</p> <pre><code>kubectl patch pvc {PVC_NAME} -p '{\"metadata\":{\"finalizers\":null}}'\n</code></pre> <p>Or if it's stuck in released:</p> <pre><code>kubectl patch pv pv-smb-tank-k8s-media-management -p '{\"spec\":{\"claimRef\": null}}'\n</code></pre>"},{"location":"cheat-sheets/siderolabs/","title":"Sidero Labs Cheat Sheet","text":"<p>Useful Omnictl &amp; Talosctl and commands geared to managing a cluster with Omni.</p>"},{"location":"cheat-sheets/siderolabs/#omni","title":"Omni","text":"<p>Quick check:</p> <pre><code>omnictl cluster status haynes-ops\n</code></pre>"},{"location":"cheat-sheets/siderolabs/#talos","title":"Talos","text":"<p>Quick check on the cluster and config:</p> <pre><code>talosctl get members\n</code></pre> <pre><code>talosctl disks\ntalosctl disks -n &lt;iNODE_IPp&gt;\n</code></pre> <p>For partitions: </p> <pre><code>talosctl get blockdevices\ntalosctl -n 192.168.40.59 get blockdevices\n</code></pre> <p>For id:</p> <pre><code>talosctl -n &lt;NODE_IP&gt; ls -l /dev/disk/by-id\n</code></pre>"},{"location":"cheat-sheets/siderolabs/#not-disks","title":"Not disks","text":"<pre><code>talosctl get links\ntalosctl get links -n &lt;NODE_IP&gt; --insecure\ntalosctl get links -o yaml\n</code></pre> <pre><code>talosctl get addresses\ntalosctl -n &lt;NODE_IP&gt; get addresses\ntalosctl get address eth0/172.20.0.2/24 -o yaml\n</code></pre> <pre><code>talosctl get addressspecs\ntalosctl -n &lt;NODE_IP&gt; get addressspecs\ntalosctl get addressspecs eth0/172.20.0.2/24 -o yaml\n</code></pre> <pre><code>talosctl get hostnamestatus\ntalosctl -n &lt;NODE_IP&gt;  get hostnamestatus\ntalosctl get hostnamespec -o yaml --namespace network-config\n</code></pre>"},{"location":"cheat-sheets/zfs/","title":"ZFS Cheatsheet","text":""},{"location":"cheat-sheets/zfs/#clean-up","title":"Clean Up","text":"<p>See what is allocating space:</p> <pre><code>zfs list -t filesystem,volume\n</code></pre> <p>Check for snapshots:</p> <pre><code>zfs list -t snapshot\n</code></pre> <p>Delete stuff that is not needed anymore:</p> <pre><code>zfs destroy rpool/data/vm-&lt;VMID&gt;-disk-&lt;DISKID&gt;\nzfs destroy rpool/data/vm-&lt;VMID&gt;-disk-&lt;DISKID&gt;@&lt;snapshot_name&gt;\n</code></pre> <p>See what else there is:</p> <pre><code>zfs list -o space\n</code></pre>"},{"location":"cluster/","title":"Omni","text":"<p>For this cluster I am taking advantage of omni from Sidero Labs to manage my Talos installation. The home-ops clusters I've reviewed rely heavily on <code>talosctl</code> to manage their clusters but I wanted to skip a few steps and for $10/month I figured it was worth a shot. After getting more familiar with Talos and Omni I do think you can live with out it but I am enjoying the auth and VPN access that comes out of the box.</p>"},{"location":"cluster/#the-journey-begins","title":"The Journey Begins","text":"<p>At this point I've gotten started but documenting as I go endes up being a mess so here's the goods:</p> <p></p> <p>I'll recap how the story went so far but first some TODOs:</p> <ul> <li>Bootstraping flux sets those limits but I'm not sure why</li> </ul> <p>Try:</p> <p>WARNING Wiping disks is for ceph. They wipe <code>/dev/nvme#</code> which happens to change all the time when talos re-images the OS. Make sure these paths are correct before running this!!!</p> <p>TODO Devcontainer needs <code>helm plugin install https://github.com/databus23/helm-diff</code> because <code>helm plugin list</code> is missing <code>diff</code> and if the initial apps fail they complain the second time that you need this diff thing! </p> <pre><code>task omni:sync\ntask talos:install-helm-apps\ntask rook:wipe-disks-talosm01\ntask rook:wipe-disks-talosm02\ntask rook:wipe-disks-talosm03\ntask flux:bootstrap\n</code></pre> <p>TODO See Rook task file and add <code>RookDiskWipe</code> in which was needed to get bluestore partition off of OSDs</p>"},{"location":"cluster/auth/","title":"Auth","text":"<p>Page for ideas on auth and eventually what I went with</p>"},{"location":"cluster/auth/#examples","title":"Examples","text":"<p>This has some cool annotations for nginx and authelia - https://github.com/mchestr/home-cluster/blob/main/kubernetes/apps/default/esphome/app/helmrelease.yaml</p> <p>May be worth seeing exactly what is going on in this cluster for netwroking etc. Cilium seems to be load balancer of choice for Talos users.</p>"},{"location":"cluster/migrate-from-proxmox/","title":"Migrating a node from Proxmox to Talos","text":"<p>Goal for the new stack is Omni controlled bare metal Talos stack.</p> <p>https://www.talos.dev/v1.7/talos-guides/install/omni/</p>"},{"location":"cluster/migrate-from-proxmox/#reclaiming-pve-node-in-k3s-cluster","title":"Reclaiming PVE node in k3s cluster","text":""},{"location":"cluster/migrate-from-proxmox/#removing-vm-from-cluster","title":"Removing VM from Cluster","text":"<pre><code>kubectl get nodes\nthaynes@HaynesHyperion:~$ kubectl get nodes\nNAME      STATUS   ROLES                       AGE   VERSION\nkubem01   Ready    control-plane,etcd,master   65d   v1.30.3+k3s1\nkubem02   Ready    control-plane,etcd,master   65d   v1.30.3+k3s1\nkubem03   Ready    control-plane,etcd,master   65d   v1.30.3+k3s1\nkubew01   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\nkubew02   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\nkubew03   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\nkubew04   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\n</code></pre> <p>Find the node you want and drain it with:</p> <pre><code>kubectl drain kubew04 --ignore-daemonsets --delete-local-data\n</code></pre> <p>Then just delete it:</p> <pre><code>kubectl delete node kubew04\n</code></pre> <p>And now it's gone!</p> <pre><code>thaynes@HaynesHyperion:~$ kubectl get nodes\nNAME      STATUS   ROLES                       AGE   VERSION\nkubem01   Ready    control-plane,etcd,master   65d   v1.30.3+k3s1\nkubem02   Ready    control-plane,etcd,master   65d   v1.30.3+k3s1\nkubem03   Ready    control-plane,etcd,master   65d   v1.30.3+k3s1\nkubew01   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\nkubew02   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\nkubew03   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\n</code></pre>"},{"location":"cluster/migrate-from-proxmox/#reclaim-nodes-from-pve","title":"Reclaim Nodes from PVE","text":"<p>Now that the k3s isn't relying on the node we can shut down or delete that VM. Then remove this node from the HA cluster and migrate all HA VMs off to other nodes.</p> <p>NOTE I also have a load balancer for the proxmox UI so I'll clean up that config to remove this</p>"},{"location":"cluster/migrate-from-proxmox/#ceph","title":"Ceph","text":"<ol> <li>Set OSDs to \"out\" and wait for Ceph to rebalance</li> <li>Destroy MGR, MON, and MDS from the UI</li> <li>Once OSDs are empty destroy them</li> </ol>"},{"location":"cluster/migrate-from-proxmox/#pve","title":"PVE","text":"<p>Once nothing is running on the node we can follow these steps to remove nodes from proxmox. </p> <p>Delete the node with:</p> <pre><code>pvecm delnode &lt;NODE&gt;\n</code></pre> <p>Then clean it out here:</p> <pre><code>root@pve01:/etc/pve/nodes# cd /etc/pve/nodes\nroot@pve01:/etc/pve/nodes# rm -R pve05/\n</code></pre> <p>You can also clean the node itself out by deleting:</p> <pre><code>systemctl stop pve-cluster corosync\npmxcfs -l\nrm /etc/corosync/*\nrm /etc/pve/corosync.conf\nkillall pmxcfs\nsystemctl start pve-cluster\n</code></pre> <p>And <code>rm -R /etc/pve/nodes</code> but I didn't get that far.</p>"},{"location":"cluster/migrate-from-proxmox/#update-ms-01-bios","title":"Update MS-01 BIOS","text":"<p>While I'm at it there's a BIOS update to apply.</p> <p>First upgrade BIOS for MS-01. - tutorial  - download</p>"},{"location":"cluster/migrate-from-proxmox/#clean-drives","title":"Clean Drives","text":"<p>Boot using Gparted and delete any data on the drives we will be using. This is especially important for Ceph as that is picky and hard to fix up the drives from Talos. </p> <p>Once you add the node to the cluster, before configurign Ceph, run this to wipe the partition table:</p> <pre><code>$ cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: disk-wipe\n  namespace: rook-ceph\nspec:\n  restartPolicy: Never\n  nodeName: talosm01\n  containers:\n  - name: disk-wipe\n    image: busybox\n    securityContext:\n      privileged: true\n    command: [\"/bin/sh\", \"-c\", \"dd if=/dev/zero bs=1M count=100 oflag=direct of=/dev/nvme0n1\"]\nEOF\npod/disk-wipe created\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-wipe\npod/disk-wipe condition met\n\n$ kubectl delete pod disk-wipe\npod \"disk-wipe\" deleted\n</code></pre>"},{"location":"cluster/pikvm-readme/","title":"PiKVM","text":"<p>See <code>./pikvm.yaml</code> for example config - I am manually keeping this in sync with what is on the device.</p>"},{"location":"cluster/pikvm-readme/#editing-config","title":"Editing Config","text":"<p>To edit config in pikvm terminal:</p> <pre><code>su -\nrw\nnano /etc/kvmd/override.yaml\nro\nexit\n</code></pre>"},{"location":"cluster/pikvm-readme/#configure-tesmart","title":"Configure TeSMART","text":"<p>TODO this was tricky document before moving</p>"},{"location":"cluster/pikvm-readme/#configure-wol","title":"Configure WOL","text":""},{"location":"cluster/pikvm-readme/#monitoring","title":"Monitoring","text":"<p>See notes here which I have copied below.</p>"},{"location":"cluster/pikvm-readme/#monitoring_1","title":"Monitoring","text":"<p>This is done ON the KVM! </p>"},{"location":"cluster/pikvm-readme/#install-node-exporter","title":"Install node-exporter","text":"<pre><code>pacman -S prometheus-node-exporter\nsystemctl enable --now prometheus-node-exporter\n</code></pre>"},{"location":"cluster/pikvm-readme/#install-promtail","title":"Install promtail","text":"<ol> <li> <p>Install promtail</p> <pre><code>pacman -S promtail\nsystemctl enable promtail\n</code></pre> </li> <li> <p>Override the promtail systemd service</p> <pre><code>mkdir -p /etc/systemd/system/promtail.service.d/\ncat &gt;/etc/systemd/system/promtail.service.d/override.conf &lt;&lt;EOL\n[Service]\nType=simple\nExecStart=\nExecStart=/usr/bin/promtail -config.file /etc/loki/promtail.yaml\nEOL\n</code></pre> </li> <li> <p>Add or replace the file <code>/etc/loki/promtail.yaml</code></p> <pre><code>server:\n  log_level: info\n  disable: true\n\nclient:\n  url: \"https://loki.devbu.io/loki/api/v1/push\"\n\npositions:\n  filename: /tmp/positions.yaml\n\nscrape_configs:\n  - job_name: journal\n    journal:\n      path: /run/log/journal\n      max_age: 12h\n      labels:\n        job: systemd-journal\n    relabel_configs:\n      - source_labels: [\"__journal__systemd_unit\"]\n        target_label: unit\n      - source_labels: [\"__journal__hostname\"]\n        target_label: hostname\n</code></pre> </li> <li> <p>Start promtail</p> <pre><code>systemctl daemon-reload\nsystemctl enable --now promtail.service\n</code></pre> </li> </ol>"},{"location":"cluster/rook-ceph/","title":"Rook Ceph on Omni","text":""},{"location":"cluster/rook-ceph/#todo","title":"TODO","text":"<p>Check out command here that uses MODEL to look up disk instead of /dev/nvme which has been changing around for this cluster.</p>"},{"location":"cluster/rook-ceph/#wtf","title":"WTF","text":"<p>Tip</p> <ul> <li>External Secrets (for dashboard)</li> </ul> <p>A cluster insn't a cluster with out some ceph action and that's where this rook guide on the talos docs comes in. </p> What does this do? <p>This seems cool but we will come back to it later.</p> <p>=== \"One thing\"</p> <pre><code>This is one thing to check out.\n</code></pre> <p>=== \"A second thing\"</p> <pre><code>This is a second thing to check out.\n</code></pre> <p>Back to ceph! </p>"},{"location":"cluster/rook-ceph/#fstrim","title":"fstrim","text":"<p>See Bernd-home-ops, has fstrim service runing</p> <pre><code>fstrim is a command-line utility in Linux used to discard (or \"trim\") unused blocks on a mounted filesystem. When files are deleted or moved, the filesystem updates its metadata to mark those blocks as free. However, the underlying storage device may not be aware that these blocks are no longer in use. Running fstrim informs the storage device about the unused blocks, allowing it to manage its storage space more efficiently.\n</code></pre>"},{"location":"cluster/wishlist/","title":"Wishlist","text":"<ul> <li>Glance</li> <li>Paperless</li> <li>hajimari as seen in action here</li> </ul> <pre><code>          hajimari.io/enable: \"true\"\n          hajimari.io/icon: cib:visual-studio-code\n          hajimari.io/group: \"home automation\"\n          hajimari.io/appName: \"ESPHome Configuration\"\n          hajimari.io/instance: \"admin\"\n</code></pre> <p>OR</p> <ul> <li>homepage   See https://github.com/brunnels/talos-cluster/blob/main/kubernetes/apps/automation/esphome/app/helmrelease.yaml#L75C11-L75C26</li> </ul>"},{"location":"home-automation/","title":"Home Automation","text":"<p>These documents are going to focus more on getting services up an running and less about what hardware and automations I'm using. That shall be documented elsewhere.</p> <p>For now this will be a placeholder / TODO list...</p> <ul> <li>Flash Tubesz Z-Wave PoE Kit with firmware zw-kit</li> <li>Flash TubesZB EFR32 MGM24 PoE Coordinator 2024 with firmware efr32-MGM24</li> </ul>"},{"location":"home-automation/#flash-zigbee-efr32-coordinatior","title":"Flash Zigbee EFR32 Coordinatior","text":"<p>I believe there are two pieces to the firmware - one is the silicon labs coordinator while the other is the ESP32 firmware on the board running it.</p>"},{"location":"home-automation/#silicon-labs-for-zigbee","title":"Silicon Labs for Zigbee","text":"<ol> <li>Install HAOS Addon (while we stil can) on the VM instance</li> <li>Use IP/Port for device -&gt; 192.168.50.162:6638</li> <li>Click usb device even though it's not usb</li> <li>Grab the link to the 'raw' view of the file in gitlab, in this case https://github.com/tube0013/tube_gateways/raw/refs/heads/main/models/current/tubeszb-efr32-MGM24/firmware/mgm24/ncp/4.4.3/tubesZB-EFR32-MGM24_NCP_7.4.3.gbl - the link the the file itself is just a webpage</li> <li>Use default baudrate </li> <li>Verbose mode becasue why not</li> <li>Save</li> <li>Accept the restart</li> <li>Watch logs</li> </ol> <p>Looks fine but we won't know until we get Z2M back alive:</p> <pre><code>[21:43:11] INFO: universal-silabs-flasher-up script exited with code 0\ns6-rc: info: service universal-silabs-flasher successfully started\ns6-rc: info: service legacy-services: starting\ns6-rc: info: service legacy-services successfully started\ns6-rc: info: service legacy-services: stopping\ns6-rc: info: service legacy-services successfully stopped\ns6-rc: info: service universal-silabs-flasher: stopping\ns6-rc: info: service universal-silabs-flasher successfully stopped\ns6-rc: info: service banner: stopping\ns6-rc: info: service banner successfully stopped\ns6-rc: info: service legacy-cont-init: stopping\ns6-rc: info: service legacy-cont-init successfully stopped\ns6-rc: info: service fix-attrs: stopping\ns6-rc: info: service fix-attrs successfully stopped\ns6-rc: info: service s6rc-oneshot-runner: stopping\ns6-rc: info: service s6rc-oneshot-runner successfully stopped\n</code></pre>"},{"location":"home-automation/#esphome-for-zigbee","title":"ESPHome for Zigbee","text":"<p>Hostname tubeszb-zigbee01.local</p> <p>Looks ready to go on <code>tubeszb-zigbee01.haynesnetwork:6053</code> though 6638 is the port for zigbee so not sure what 6638 is for. Has a wabsite at <code>http://tubeszb-zigbee01.haynesnetwork/</code></p>"},{"location":"home-automation/#flash-z-wave-kit","title":"Flash Z-Wave Kit","text":""},{"location":"home-automation/#zooz-radeo-firmware","title":"Zooz Radeo Firmware","text":"<p>ZWave JS UI is complainign that it want's Z-Wave SDK 7.22.1 or greater so we better do this one!</p> <p>Get the file from Zooz, at the time I did so it was here and called <code>https://www.getzooz.com/firmware/ZAC93_SDK_7.22.1_US-LR_V01R50.zip</code>. Note that I have model number <code>ZAC93</code> which is how I found this file.</p> <p>Extract the zip and verify it's a .gbl file. Then go to the device in ZWave JS UI -&gt; Advanced -&gt; Firmware update OTW and select the file.</p> <p>Note that the release notes stopped being updated at 1.20 but here in October we got 1.50</p>"},{"location":"home-automation/#esphome-for-z-wave","title":"ESPHome for Z Wave","text":"<p>Hostname tubeszb-zwave01.local</p> <p>This looks different and the firmware files look esphome related. I will use esphome to update it!</p> <p>ESPHome discovered both of these guys. After clicking Adop it just rocked and rolled. Some encryption key was spat out and I had the option to update it.</p> <p>Well, update upataded, and it seemed to point at the repo I was going to go to anyway. Now it has a website! <code>http://tubeszb-zwave01.haynesnetwork</code> and <code>tubeszb-zwave01.haynesnetwork:6638</code> seems ready to go... Guess I'll delete it for now and see how setting other stuff up goes.</p>"},{"location":"home-automation/backlog/","title":"Home Automation Backlog","text":""},{"location":"home-automation/backlog/#hass","title":"HASS","text":"<ul> <li>Setup Recorder</li> </ul>"},{"location":"home-automation/voice-assist/","title":"Voice Assistant","text":"<p>TODO </p> <ul> <li>https://github.com/acon96/home-llm</li> </ul>"},{"location":"observability/","title":"Observability","text":"<p>Do this FIRST to save time later (said noone)</p>"},{"location":"observability/#todo","title":"TODO","text":"<ul> <li>Setup kromogo</li> <li>Use kromogo on github readme after I migrate the cluster and use the other public IP</li> <li>review smartctl-exporter</li> <li>review snmp-exporter</li> <li>review unpoller</li> <li>Gatus templates need fixing</li> <li>I did not include or turned off out a bunch of stuff since the observability stack wasn't set up. I need to review resources to see what needs to be enabled</li> <li>Grafana and initial dashboards</li> <li>add custom dashboards etc</li> </ul>"},{"location":"observability/#dashboards-todo","title":"Dashboards TODO","text":"<ul> <li>Traefik Official</li> </ul>"},{"location":"observability/namespace-alerts/","title":"Namespace Alerts","text":"<p>I keep deleting stuff like this from <code>rook-ceph</code>'s namespace:</p> <pre><code>---\n# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/notification.toolkit.fluxcd.io/provider_v1beta3.json\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Provider\nmetadata:\n  name: alert-manager\n  namespace: rook-ceph\nspec:\n  type: alertmanager\n  address: http://alertmanager-operated.observability.svc.cluster.local:9093/api/v2/alerts/\n---\n# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/notification.toolkit.fluxcd.io/alert_v1beta3.json\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Alert\nmetadata:\n  name: alert-manager\n  namespace: rook-ceph\nspec:\n  providerRef:\n    name: alert-manager\n  eventSeverity: error\n  eventSources:\n    - kind: HelmRelease\n      name: \"*\"\n  exclusionList:\n    - \"error.*lookup github\\\\.com\"\n    - \"error.*lookup raw\\\\.githubusercontent\\\\.com\"\n    - \"dial.*tcp.*timeout\"\n    - \"waiting.*socket\"\n  suspend: false\n</code></pre> <p>Need to get alerts going right after barebones smart home. </p>"},{"location":"observability/snmp-monitoring/","title":"SNMP Monitoring","text":"<p>I am skipping snmp_exporter for now as I do not have any devices setup to produce data it can scrape. However, I have things I can setup later:</p> <ul> <li>For Unraid we need to follow a setup guide we have some extra setup required.</li> <li>Unifi also needs a custom config to get it ready.</li> <li>APC Ups is configured here and we can do this once we get the rack one at the new place.</li> </ul>"},{"location":"observability/snmp-monitoring/#unifi","title":"Unifi","text":"<p>We may want to skip snmp for unifi and go straight to unpoller which I'll also put on the back burner for now.</p>"},{"location":"secrets/","title":"Sectets","text":"<p>This </p>"},{"location":"storage/snapshot-upgrade/","title":"Snapshot upgrade","text":"<p>cephfilesystemsubvolumegroups.ceph.rook.io                 2024-10-31T20:43:22Z volumegroupsnapshotclasses.groupsnapshot.storage.k8s.io    2025-01-19T02:24:00Z volumegroupsnapshotcontents.groupsnapshot.storage.k8s.io   2024-10-31T20:43:01Z volumegroupsnapshots.groupsnapshot.storage.k8s.io          2024-10-31T20:43:01Z</p> <p>See this thread:</p> <pre><code>kubectl patch crd volumegroupsnapshots.groupsnapshot.storage.k8s.io --subresource='status' --type='merge' -p '{\"status\":{\"storedVersions\": [\"v1beta1\"]}}'\n</code></pre>"},{"location":"the-move/checklist/","title":"Checklist","text":""},{"location":"the-move/checklist/#router","title":"Router","text":"<ul> <li>[ ] Setup same subnets and VLANs</li> <li>[ ] Setup HNET+ WiFi for 5Ghz and 6Ghz </li> <li>[ ] Setup HNETIoT WiFi for 2.4Ghz and in the IoT VLAN. Only add 2.4Ghz - make sure the password matches (in esphome 1Password)</li> <li>[ ] Channel Width 20 for 2.4 Ghz, TBD if we use Auto for channel</li> <li>[ ] Channel Width 80 for 5Ghz &amp; Auto Channel</li> <li>[ ] Channel Width 160 for 6Ghz &amp; Auto Channel </li> <li>[ ] Sonos Hoops</li> <li>[ ] Host (A) DNS for tubeszb-zigbee01.haynesnetwork -&gt; 192.168.50.162</li> <li>[ ] Host (A) DNS for tubeszb-zwave01.haynesnetwork -&gt; 192.168.50.92</li> <li>[ ] Host (A) DNS for internal.haynesops -&gt; 192.168.40.203</li> <li>[ ] Setup external for cloudflare and forward ports etc (TODO document this)</li> </ul> <p>TODO Review the rest of the DNS stuff</p> <ul> <li>[ ] Sort out Talos01-03 IPs</li> </ul>"},{"location":"the-move/checklist/#appendix","title":"Appendix","text":""},{"location":"the-move/checklist/#vlans","title":"VLANs","text":"<p>| VLAN # | Subnet          | Name        | Decription                                   |  | 1      | 192.168.0.0/24  | Default     |                                              | | 2      | 192.168.20.0/24 | CephLan     | Isolated, no internet, for proxmox ceph only | | 3      | 192.168.30.0/24 | VPNLan      | Bound to https://mullvad.net/en              | | 4      | 192.168.40.0/24 | Hayneslab   | Configued wrt k8s loadbalacer pools          | | 5      | 192.168.50.0/24 | IoT         | TODO needs work to be better isolate         |  | 6      | 192.168.60.0/24 | RookLan     | Isolated, no internet, for rook-ceph only    |</p>"},{"location":"the-move/checklist/#dns-entries","title":"DNS Entries","text":""},{"location":"the-move/checklist/#hayneslab-vlam","title":"HaynesLab VLAM","text":"Record Fixed IP haynesintelligence.haynesnetwork 192.168.40.11 nas01.haynesnetwork 192.168.40.52 nut02.haynesnetwork 192.168.40.53 talosm01.haynesnetwork 192.168.40.93 talosm02.haynesnetwork 192.168.40.59 talosm03.haynesnetwork 192.168.40.10 pikvm.haynesnetwork 192.168.40.66 N/A (TESmart KVM) 192.168.40.70"},{"location":"the-move/checklist/#iot-vlan","title":"IoT VLAN","text":"Record Fixed IP tubeszb-zigbee01.haynesnetwork 192.168.50.162 tubeszb-zwave01.haynesnetwork 192.168.50.92"},{"location":"this-site/","title":"How This Site","text":"<p>This site is build using Material for MkDocs</p>"},{"location":"this-site/#cool-stuff","title":"Cool Stuff","text":"<p>References provides a guide to how to use some cool stuff. </p> <p>Admonitions are particularly interesting.</p>"}]}