{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Haynes Ops Cluster Repo","text":"<p>Welcome to the documents for my home ops style cluster repo. I've put these together so I can remember how it works but you are welcome to read them too!</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"todo-refactor/","title":"Todo refactor","text":""},{"location":"todo-refactor/#this-document-moved","title":"This document moved","text":"<p>The improvement backlog and modernization notes are now maintained here:</p> <ul> <li><code>docs/standardization/todo-refactor.md</code></li> </ul>"},{"location":"cheat-sheets/ceph/","title":"Ceph Cheat Sheet","text":"<p>Merge of my notes from proxmox and commands from this guide</p>"},{"location":"cheat-sheets/ceph/#rook","title":"Rook","text":"<p>Restart operator to re-invoke cluster init:</p> <pre><code>kubectl -n rook-ceph delete pod -l app=rook-ceph-operator\n</code></pre> <p>Get into a place you can use the regular commands:</p> <pre><code>kubectl --namespace rook-ceph exec -it deploy/rook-ceph-operator -- bash\n</code></pre> <p>TODO Move this</p> <p><pre><code>kubectl --namespace rook-ceph exec -it deploy/rook-ceph-operator -- bash\nrook multus validation run --public-network=network/multus-public --cluster-network=network/multus-ceph -n rook-ceph\n</code></pre> This leaves a MESS so you need to run:</p> <pre><code>rook multus validation cleanup --namespace rook-ceph\n</code></pre> <p>Then the next validation:</p> <pre><code>rook multus validation config converged\n</code></pre> <pre><code>Example:\n\nthaynes@HaynesHyperion:~$ kubectl --namespace rook-ceph exec -it deploy/rook-ceph-operator -- bash\nDefaulted container \"rook-ceph-operator\" out of: rook-ceph-operator, k8tz (init)\n[rook@rook-ceph-operator-69745fc466-pc95r /]$ rook multus validation run --help\n2024/10/26 10:59:57 maxprocs: Leaving GOMAXPROCS=20: CPU quota undefined\n\nRun a validation test that determines whether the current Multus and system\nconfigurations will support Rook with Multus.\n\nThis should be run BEFORE Rook is installed.\n\nThis is a fairly long-running test. It starts up a web server and many\nclients to verify that Multus network communication works properly.\n\nIt does *not* perform any load testing. Networks that cannot support high\nvolumes of Ceph traffic may still encounter runtime issues. This may be\nparticularly noticeable with high I/O load or during OSD rebalancing\n(see: https://docs.ceph.com/en/latest/architecture/#rebalancing).\nFor example, during Rook or Ceph cluster upgrade.\n\nOverride the kube config file location by setting the KUBECONFIG environment variable.\n\nUsage:\n  rook multus validation run [--public-network=&lt;nad-name&gt;] [--cluster-network=&lt;nad-name&gt;] [flags]\n\nFlags:\n      --cluster-network string                   The name of the Network Attachment Definition (NAD) that will be used for Ceph's cluster network. This should be a namespaced name in the form &lt;namespace&gt;/&lt;name&gt; if the NAD is defined in a different namespace from the cluster namespace.\n  -c, --config string                            The validation test config file to use. This cannot be used with other flags except --host-check-only.\n      --daemons-per-node int                     The number of validation test daemons to run per node. It is recommended to set this to the maximum number of Ceph daemons that can run on any node in the worst case of node failure(s). The default value is set to the worst-case value for a Rook Ceph cluster with 3 portable OSDs, 3 portable monitors, and where all optional child resources have been created with 1 daemon such that they all might run on a single node in a failure scenario. If you aren't sure what to choose for this value, add 1 for each additional OSD beyond 3. (default 19)\n      --flaky-threshold-seconds timeoutSeconds   This is the time window in which validation clients are all expected to become 'Ready' together. Validation clients are all started at approximately the same time, and they should all stabilize at approximately the same time. Once the first validation client becomes 'Ready', the tool checks that all of the remaining clients become 'Ready' before this threshold duration elapses. In networks that have connectivity issues, limited bandwidth, or high latency, clients will contend for network traffic with each other, causing some clients to randomly fail and become 'Ready' later than others. These randomly-failing clients are considered 'flaky.' Adjust this value to reflect expectations for the underlying network. For fast and reliable networks, this can be set to a smaller value. For networks that are intended to be slow, this can be set to a larger value. Additionally, for very large Kubernetes clusters, it may take longer for all clients to start, and it therefore may take longer for all clients to become 'Ready'; in that case, this value can be set slightly higher. (default 30s)\n  -h, --help                                     help for run\n      --host-check-only                          Only check that hosts can connect to the server via the public network. Do not start clients. This mode is recommended when a Rook cluster is already running and consuming the public network specified.\n  -n, --namespace string                         The namespace for validation test resources. It is recommended to set this to the namespace in which Rook's Ceph cluster will be installed. (default \"rook-ceph\")\n      --nginx-image string                       The Nginx image used for the validation server and clients. (default \"quay.io/nginx/nginx-unprivileged:stable-alpine\")\n      --public-network string                    The name of the Network Attachment Definition (NAD) that will be used for Ceph's public network. This should be a namespaced name in the form &lt;namespace&gt;/&lt;name&gt; if the NAD is defined in a different namespace from the cluster namespace.\n      --service-account string                   The name of the service account that will be used for test resources. (default \"rook-ceph-system\")\n      --timeout-minutes timeoutMinutes           The time to wait for resources to change to the expected state. For example, for the test web server to start, for test clients to become ready, or for test resources to be deleted. At longest, this may need to reflect the time it takes for client pods to to pull images, get address assignments, and then for each client to determine that its network connection is stable. Minimum: 1 minute. Recommended: 2 minutes or more. (default 3m0s)\n\nGlobal Flags:\n      --log-level string   logging level for logging/tracing output (valid values: ERROR,WARNING,INFO,DEBUG) (default \"INFO\")\n[rook@rook-ceph-operator-69745fc466-pc95r /]$ rook multus validation config --help\n2024/10/26 11:00:16 maxprocs: Leaving GOMAXPROCS=20: CPU quota undefined\nGenerate a validation test config file for different default scenarios to stdout.\n\nUsage:\n  rook multus validation config [command]\n\nAvailable Commands:\n  converged               Example config for a cluster that runs storage and user workloads on all nodes.\n  dedicated-storage-nodes Example config file for a cluster that uses dedicated storage nodes.\n  stretch-cluster         Example config file for a stretch cluster with dedicated storage nodes.\n\nFlags:\n  -h, --help   help for config\n\nGlobal Flags:\n      --log-level string   logging level for logging/tracing output (valid values: ERROR,WARNING,INFO,DEBUG) (default \"INFO\")\n\nUse \"rook multus validation config [command] --help\" for more information about a command.\n</code></pre>"},{"location":"cheat-sheets/ceph/#talos","title":"Talos","text":""},{"location":"cheat-sheets/ceph/#status","title":"Status","text":"<p>We can use a tools pod to run <code>ceph</code> commands</p>"},{"location":"cheat-sheets/ceph/#check-status","title":"Check Status","text":"<pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status\n</code></pre>"},{"location":"cheat-sheets/ceph/#clear-warn","title":"Clear Warn:","text":"<pre><code>kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph status\n</code></pre>"},{"location":"cheat-sheets/ceph/#misc-tood","title":"Misc TOOD","text":"<pre><code>kubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=0\nkubectl -n rook-ceph scale deployment rook-ceph-operator --replicas=1\n</code></pre> <pre><code>kubectl get storageclass\n\nkubectl -n rook-ceph get cephclusters rook-ceph\n\nkubectl -n rook-ceph get cephclusters\n</code></pre> <p>Pre-condtion before talos upgrade:</p> <pre><code>kubectl -n rook-ceph wait --timeout=1800s --for=jsonpath='{.status.ceph.health}=HEALTH_OK' rook-ceph\n</code></pre>"},{"location":"cheat-sheets/ceph/#remove-from-k8s","title":"Remove from k8s","text":"<pre><code>kubectl -n rook-ceph patch cephcluster rook-ceph --type merge -p '{\"spec\":{\"cleanupPolicy\":{\"confirmation\":\"yes-really-destroy-data\"}}}'\n\nkubectl delete storageclasses ceph-block ceph-bucket ceph-filesystem\n\nkubectl -n rook-ceph delete cephblockpools ceph-blockpool\n\nkubectl -n rook-ceph delete cephobjectstore ceph-objectstore\n\nkubectl -n rook-ceph delete cephfilesystem ceph-filesystem\n</code></pre> <p>Now delete cluster:</p> <pre><code>kubectl -n rook-ceph delete cephcluster rook-ceph\n\nhelm -n rook-ceph uninstall rook-ceph-cluster\n</code></pre> <p>Now the operator: <pre><code>helm -n rook-ceph uninstall rook-ceph\n</code></pre></p>"},{"location":"cheat-sheets/ceph/#cluseter-finalizer","title":"Cluseter Finalizer","text":"<pre><code>kubectl patch cephcluster rook-ceph -n rook-ceph --type=json -p='[{\"op\": \"remove\", \"path\": \"/metadata/finalizers\"}]'\n</code></pre> <pre><code>for CRD in $(kubectl get crd -n rook-ceph | awk '/ceph.rook.io/ {print $1}'); do\n    kubectl get -n rook-ceph \"$CRD\" -o name | \\\n    xargs -I {} kubectl patch -n rook-ceph {} --type merge -p '{\"metadata\":{\"finalizers\": []}}'\ndone\n</code></pre> <pre><code>kubectl -n rook-ceph patch configmap rook-ceph-mon-endpoints --type merge -p '{\"metadata\":{\"finalizers\": []}}'\nkubectl -n rook-ceph patch secrets rook-ceph-mon --type merge -p '{\"metadata\":{\"finalizers\": []}}'\n</code></pre>"},{"location":"cheat-sheets/ceph/#finish-off-metadata","title":"Finish off metadata","text":"<p>Talos may need some massaging:</p> <pre><code>$ cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: disk-clean\nspec:\n  restartPolicy: Never\n  nodeName: &lt;storage-node-name&gt;\n  volumes:\n  - name: rook-data-dir\n    hostPath:\n      path: &lt;dataDirHostPath&gt;\n  containers:\n  - name: disk-clean\n    image: busybox\n    securityContext:\n      privileged: true\n    volumeMounts:\n    - name: rook-data-dir\n      mountPath: /node/rook-data\n    command: [\"/bin/sh\", \"-c\", \"rm -rf /node/rook-data/*\"]\nEOF\npod/disk-clean created\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-clean\npod/disk-clean condition met\n\n$ kubectl delete pod disk-clean\npod \"disk-clean\" deleted\n</code></pre> <p>And wipe the disks:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: disk-wipe\n  namespace: rook-ceph\nspec:\n  restartPolicy: Never\n  nodeName: talosm03\n  containers:\n  - name: disk-wipe\n    image: busybox\n    securityContext:\n      privileged: true\n    command: [\"/bin/sh\", \"-c\", \"dd if=/dev/zero bs=1M count=100 oflag=direct of=/dev/nvme1n1\"]\nEOF\npod/disk-wipe created\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-wipe\npod/disk-wipe condition met\n\n$ kubectl delete pod disk-wipe\npod \"disk-wipe\" deleted\n</code></pre>"},{"location":"cheat-sheets/ceph/#ceph","title":"Ceph","text":"<p>Not sure how these would work unless you install ceph and a config that points to the cluster on another machine (seeing you can't run ceph cli on talos)</p>"},{"location":"cheat-sheets/ceph/#archive-crash-warnings","title":"Archive crash warnings","text":"<p>These happen when I reboot.</p> <pre><code>ceph crash archive-all\n</code></pre>"},{"location":"cheat-sheets/ceph/#cluster-info","title":"Cluster info","text":""},{"location":"cheat-sheets/ceph/#status_1","title":"Status","text":"<p>Overall status of the cluster.</p> <pre><code>ceph status || ceph -w\n</code></pre>"},{"location":"cheat-sheets/ceph/#config","title":"Config","text":"<pre><code>ceph config dump\n</code></pre>"},{"location":"cheat-sheets/ceph/#monitors","title":"Monitors","text":"<p>Get details about the monitors.</p> <pre><code>ceph mon dump\n</code></pre>"},{"location":"cheat-sheets/ceph/#ceph-services","title":"Ceph Services","text":""},{"location":"cheat-sheets/ceph/#see-services","title":"See Services","text":"<pre><code>ceph mgr services\n</code></pre>"},{"location":"cheat-sheets/ceph/#restart-service","title":"Restart Service","text":"<pre><code>ceph mgr module disable dashboard\nceph mgr module enable dashboard\n</code></pre>"},{"location":"cheat-sheets/kubernetes/","title":"Kubernetes Cheat Sheet","text":"<p>Find api resources for namespace</p> <pre><code>kubectl api-resources --verbs=list --namespaced -o name \\\n  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n &lt;namespace&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#flux","title":"Flux","text":"<pre><code>kubectl logs -n flux-system deploy/helm-controller\n</code></pre> <p>title: Kubernetes Cheat Sheet permalink: /cheatsheets/kube-commands/</p>"},{"location":"cheat-sheets/kubernetes/#retries-exceeded","title":"Retries Exceeded","text":"<p>I see this when things take a while because something else is broken (like a csi) but there is no reason to fix the thing that is stuck. First force helmrelease to get fixed:</p> <pre><code>flux reconcile helmrelease -n home-automation home-assistant --force\n</code></pre> <p>Then get the kustomization in a good state again (if the helmrelease was OK):</p> <pre><code>flux reconcile kustomization -n flux-system zigbee2mqtt\n</code></pre> <p>Here are some commands for managing Kubernetes.</p>"},{"location":"cheat-sheets/kubernetes/#clean-mess","title":"Clean Mess","text":"<p>Check what you are deleting:</p> <pre><code>kubectl get pod --field-selector=status.phase==Succeeded -A\nkubectl get pod --field-selector=status.phase==Failed -A\n</code></pre> <p>If nothing to debug go ahead and kill em:</p> <pre><code>kubectl delete pod --field-selector=status.phase==Succeeded -A\nkubectl delete pod --field-selector=status.phase==Failed -A\n</code></pre> <p>Rebooting sometimes creates a mess and this will clean it but I thing they go away after a bit.</p>"},{"location":"cheat-sheets/kubernetes/#dns","title":"DNS","text":"<p>DNS Test:</p> <pre><code>kubectl -n ai run dns-test --rm -it --image=busybox --restart=Never -- nslookup volsync-hayesops.s3.amazonaws.com\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectx-kubens","title":"kubectx &amp; kubens","text":"<p>These are helper scripts that come from here.</p>"},{"location":"cheat-sheets/kubernetes/#show-available-contexts-clusters","title":"Show available contexts (clusters)","text":"<pre><code>kubectx\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#switch-cluster","title":"Switch cluster","text":"<pre><code>kubectx &lt;cluster name&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#show-available-namespaces","title":"Show available namespaces","text":"<pre><code>kubens\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#switch-namespaces","title":"Switch namespaces","text":"<pre><code>kubens &lt;namespace&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectl","title":"kubectl","text":"<p>This is the beast that drives operations around your k8s cluster. Most of the heavy lifting seems to be done in the manifest yaml or helm charts but debugging and/or setting up requires heavy use of <code>kubectl</code>.</p>"},{"location":"cheat-sheets/kubernetes/#kubectl-apply","title":"kubectl apply","text":"<pre><code>kubectl apply -f &lt;filename.yaml&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectl-get","title":"kubectl get","text":"<p>To see what's running, installed, or anything else we use <code>kubectl get &lt;thing&gt;</code></p>"},{"location":"cheat-sheets/kubernetes/#get-nodes","title":"get nodes","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-namespaces","title":"get namespaces","text":"<pre><code>kubectl get namespaces\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-pods","title":"get pods","text":"<pre><code>kubectl get pods --all-namespaces\nkubectl -n &lt;namespace&gt; kubget pods \n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-services","title":"get services","text":"<p>This is useful for figuring port mappings.</p> <pre><code>kubectl get svc --all-namespaces\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-volumes","title":"get volumes","text":"<pre><code>kubectl get pv,pvc -o wide\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#browse-volume","title":"Browse Volume","text":"<pre><code>kubectl browse-pvc -n home-automation zigbee2mqtt\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-volumesnapshots","title":"get volumesnapshots","text":"<pre><code>kubectl get volumesnapshot\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-volumesnapshotclass","title":"get volumesnapshotclass","text":"<p>This will show you what type of snapshots you can take.</p> <pre><code>kubectl get volumesnapshotclass\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#get-ceph-cluster","title":"get ceph cluster","text":"<p>Since I used rook this tells me how the ceph cluster exposed to k8s is doing.</p> <pre><code>kubectl -n rook-ceph  get CephCluster\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectl-delete","title":"kubectl delete","text":""},{"location":"cheat-sheets/kubernetes/#delete-manifest","title":"delete manifest","text":"<pre><code>kubectl delete -f &lt;manifest.yaml&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#delete-all-your-volumes-dont-do-this","title":"delete all your volumes (don't do this!)","text":"<pre><code>kubectl delete pvc --all\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#delete-pv","title":"delete pv","text":"<pre><code>kubectl patch pvc -n namespace PVCNAME -p '{\"metadata\": {\"finalizers\": null}}'\nkubectl patch pv PVNAME -p '{\"metadata\": {\"finalizers\": null}}'\n\nk delete pvc PVCNAME --grace-period=0 --force\nk delete pv PVNAME --grace-period=0 --force\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectl-describe","title":"kubectl describe","text":"<p>Describe things in the cluster like pods and pvc's. This will give you some basic info to troubleshoot with before going to the logs (or if the logs are gone because the thing is crash looping.</p> <pre><code>kubectl describe &lt;type&gt; &lt;name&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectl-logs","title":"kubectl logs","text":"<p>For an app:</p> <pre><code>kubectl logs -n velero -l app.kubernetes.io/name=velero\n</code></pre> <p>For a pod:</p> <pre><code>kubectl logs &lt;podname&gt;\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#kubectl-exec","title":"kubectl exec","text":"<p>Like bashing into a docker container but for a pod. Note if you are not kubens'd into the namespace you will need <code>-n &lt;namespace&gt;</code></p> <pre><code>kubectl exec -it -n &lt;namespace&gt; &lt;pod_name&gt; -- env\n</code></pre> <p>Bash in:</p> <pre><code>k exec -it -n &lt;namespace&gt;  &lt;pod_name&gt; -- sh\nk exec -it  -n &lt;namespace&gt;  &lt;pod_name&gt; -- /bin/bash\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#restart","title":"Restart","text":"<pre><code>https://kubernetes.io/docs/reference/kubectl/generated/kubectl_rollout/kubectl_rollout_restart/\n\nk rollout restart deployment/prowlarr -n media-management\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#see-what-capabilities-you-have","title":"See what capabilities you have","text":"<p>This will show resources. Here we are checking to see if the cluster can make volumesnapshots or at least is eligible to be setup for volumesnapshots.</p> <pre><code>kubectl api-resources | grep volumesnapshots\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#velero","title":"Velero","text":""},{"location":"cheat-sheets/kubernetes/#create-backup","title":"Create Backup","text":"<pre><code>velero backup create hellothere --include-namespaces=default --wait\n</code></pre> <pre><code> velero backup create --from-schedule=velero-daily-backups\n ```\n\n#### TODO FROM DEBUGGING NEEDS ORGANIZING\n\n```bash\nkubectl get events -A\nceph -s\nceph osd lspools\nkubectl logs -f velero-764d58dfd9-k47sh\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#mass-delete","title":"Mass Delete","text":""},{"location":"cheat-sheets/kubernetes/#namespace-w-spec-finalizer","title":"Namespace w/ Spec Finalizer","text":"<p>This was super hard to get rid of vs. a regular namespace! </p> <pre><code>kubectl get ns rook-ceph -o json | jq '.spec.finalizers = []' | kubectl replace --raw \"/api/v1/namespaces/rook-ceph/finalize\" -f -\n</code></pre> <pre><code>kubectl get namespace rook-ceph -o json &gt; tmp.json\n</code></pre> <ul> <li>Delete kubernetes finalizer in tmp.json (leave empty array \"finalizers\": [])</li> <li>Run kubectl proxy in another terminal for auth purposes and run following curl request to returned port</li> </ul> <pre><code>kubectl proxy &amp;\ncurl -k -H \"Content-Type: application/json\" -X PUT --data-binary @tmp.json 127.0.0.1:8001/api/v1/namespaces/rook-ceph/finalize\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#jobs","title":"Jobs","text":"<pre><code>kubectl get pod  -n velero --field-selector=status.phase==Succeeded\n\nkubectl delete pod -n velero  --field-selector=status.phase==Succeeded\n\nkubectl delete pod -n velero  --field-selector=status.phase==Failed\n</code></pre> <pre><code>kubectl get jobs  -n velero --field-selector status.successful=1\n\nkubectl delete jobs -n velero --field-selector status.successful=1\n</code></pre> <p>Pass <code>`--all-namespaces</code> for a good time!</p> <pre><code>velero delete backup move-data-test -n velero\nvelero delete backup velero-daily-backups-20240827000047 -n velero\nvelero delete backup velero-daily-backups-20240826000046 -n velero\n</code></pre> <p>TANK CHECK:</p> <pre><code>velero describe backup move-data-test-smb --details\n</code></pre> <pre><code>kubectl -n velero rollout restart daemonset node-agent\nkubectl -n velero rollout restart deployment velero\n</code></pre>"},{"location":"cheat-sheets/kubernetes/#stuck-stuff","title":"Stuck stuff","text":"<pre><code>kubectl get volumeattachment\n</code></pre> <p>See if it's attached, remove finalizes:</p> <pre><code>kubectl patch pvc {PVC_NAME} -p '{\"metadata\":{\"finalizers\":null}}'\n</code></pre> <p>Or if it's stuck in released:</p> <pre><code>kubectl patch pv pv-smb-tank-k8s-media-management -p '{\"spec\":{\"claimRef\": null}}'\n</code></pre>"},{"location":"cheat-sheets/siderolabs/","title":"Sidero Labs Cheat Sheet","text":"<p>Useful Omnictl &amp; Talosctl and commands geared to managing a cluster with Omni.</p>"},{"location":"cheat-sheets/siderolabs/#omni","title":"Omni","text":"<p>Quick check:</p> <pre><code>omnictl cluster status haynes-ops\n</code></pre>"},{"location":"cheat-sheets/siderolabs/#talos","title":"Talos","text":"<p>Quick check on the cluster and config:</p> <pre><code>talosctl get members\n</code></pre> <pre><code>talosctl disks\ntalosctl disks -n &lt;iNODE_IPp&gt;\n</code></pre> <p>For partitions: </p> <pre><code>talosctl get blockdevices\ntalosctl -n 192.168.40.59 get blockdevices\n</code></pre> <p>For id:</p> <pre><code>talosctl -n &lt;NODE_IP&gt; ls -l /dev/disk/by-id\n</code></pre>"},{"location":"cheat-sheets/siderolabs/#not-disks","title":"Not disks","text":"<pre><code>talosctl get links\ntalosctl get links -n &lt;NODE_IP&gt; --insecure\ntalosctl get links -o yaml\n</code></pre> <pre><code>talosctl get addresses\ntalosctl -n &lt;NODE_IP&gt; get addresses\ntalosctl get address eth0/172.20.0.2/24 -o yaml\n</code></pre> <pre><code>talosctl get addressspecs\ntalosctl -n &lt;NODE_IP&gt; get addressspecs\ntalosctl get addressspecs eth0/172.20.0.2/24 -o yaml\n</code></pre> <pre><code>talosctl get hostnamestatus\ntalosctl -n &lt;NODE_IP&gt;  get hostnamestatus\ntalosctl get hostnamespec -o yaml --namespace network-config\n</code></pre>"},{"location":"cheat-sheets/zfs/","title":"ZFS Cheatsheet","text":""},{"location":"cheat-sheets/zfs/#clean-up","title":"Clean Up","text":"<p>See what is allocating space:</p> <pre><code>zfs list -t filesystem,volume\n</code></pre> <p>Check for snapshots:</p> <pre><code>zfs list -t snapshot\n</code></pre> <p>Delete stuff that is not needed anymore:</p> <pre><code>zfs destroy rpool/data/vm-&lt;VMID&gt;-disk-&lt;DISKID&gt;\nzfs destroy rpool/data/vm-&lt;VMID&gt;-disk-&lt;DISKID&gt;@&lt;snapshot_name&gt;\n</code></pre> <p>See what else there is:</p> <pre><code>zfs list -o space\n</code></pre>"},{"location":"cluster/","title":"Omni","text":"<p>For this cluster I am taking advantage of omni from Sidero Labs to manage my Talos installation. The home-ops clusters I've reviewed rely heavily on <code>talosctl</code> to manage their clusters but I wanted to skip a few steps and for $10/month I figured it was worth a shot. After getting more familiar with Talos and Omni I do think you can live with out it but I am enjoying the auth and VPN access that comes out of the box.</p>"},{"location":"cluster/#the-journey-begins","title":"The Journey Begins","text":"<p>At this point I've gotten started but documenting as I go endes up being a mess so here's the goods:</p> <p></p> <p>I'll recap how the story went so far but first some TODOs:</p> <ul> <li>Bootstraping flux sets those limits but I'm not sure why</li> </ul> <p>Try:</p> <p>WARNING Wiping disks is for ceph. They wipe <code>/dev/nvme#</code> which happens to change all the time when talos re-images the OS. Make sure these paths are correct before running this!!!</p> <p>TODO Devcontainer needs <code>helm plugin install https://github.com/databus23/helm-diff</code> because <code>helm plugin list</code> is missing <code>diff</code> and if the initial apps fail they complain the second time that you need this diff thing! </p> <pre><code>task omni:sync\ntask talos:install-helm-apps\ntask rook:wipe-disks-talosm01\ntask rook:wipe-disks-talosm02\ntask rook:wipe-disks-talosm03\ntask flux:bootstrap\n</code></pre> <p>TODO See Rook task file and add <code>RookDiskWipe</code> in which was needed to get bluestore partition off of OSDs</p>"},{"location":"cluster/auth/","title":"Auth","text":"<p>Page for ideas on auth and eventually what I went with</p>"},{"location":"cluster/auth/#examples","title":"Examples","text":"<p>This has some cool annotations for nginx and authelia - https://github.com/mchestr/home-cluster/blob/main/kubernetes/apps/default/esphome/app/helmrelease.yaml</p> <p>May be worth seeing exactly what is going on in this cluster for netwroking etc. Cilium seems to be load balancer of choice for Talos users.</p>"},{"location":"cluster/migrate-from-proxmox/","title":"Migrating a node from Proxmox to Talos","text":"<p>Goal for the new stack is Omni controlled bare metal Talos stack.</p> <p>https://www.talos.dev/v1.7/talos-guides/install/omni/</p>"},{"location":"cluster/migrate-from-proxmox/#reclaiming-pve-node-in-k3s-cluster","title":"Reclaiming PVE node in k3s cluster","text":""},{"location":"cluster/migrate-from-proxmox/#removing-vm-from-cluster","title":"Removing VM from Cluster","text":"<pre><code>kubectl get nodes\nthaynes@HaynesHyperion:~$ kubectl get nodes\nNAME      STATUS   ROLES                       AGE   VERSION\nkubem01   Ready    control-plane,etcd,master   65d   v1.30.3+k3s1\nkubem02   Ready    control-plane,etcd,master   65d   v1.30.3+k3s1\nkubem03   Ready    control-plane,etcd,master   65d   v1.30.3+k3s1\nkubew01   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\nkubew02   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\nkubew03   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\nkubew04   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\n</code></pre> <p>Find the node you want and drain it with:</p> <pre><code>kubectl drain kubew04 --ignore-daemonsets --delete-local-data\n</code></pre> <p>Then just delete it:</p> <pre><code>kubectl delete node kubew04\n</code></pre> <p>And now it's gone!</p> <pre><code>thaynes@HaynesHyperion:~$ kubectl get nodes\nNAME      STATUS   ROLES                       AGE   VERSION\nkubem01   Ready    control-plane,etcd,master   65d   v1.30.3+k3s1\nkubem02   Ready    control-plane,etcd,master   65d   v1.30.3+k3s1\nkubem03   Ready    control-plane,etcd,master   65d   v1.30.3+k3s1\nkubew01   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\nkubew02   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\nkubew03   Ready    &lt;none&gt;                      65d   v1.30.3+k3s1\n</code></pre>"},{"location":"cluster/migrate-from-proxmox/#reclaim-nodes-from-pve","title":"Reclaim Nodes from PVE","text":"<p>Now that the k3s isn't relying on the node we can shut down or delete that VM. Then remove this node from the HA cluster and migrate all HA VMs off to other nodes.</p> <p>NOTE I also have a load balancer for the proxmox UI so I'll clean up that config to remove this</p>"},{"location":"cluster/migrate-from-proxmox/#ceph","title":"Ceph","text":"<ol> <li>Set OSDs to \"out\" and wait for Ceph to rebalance</li> <li>Destroy MGR, MON, and MDS from the UI</li> <li>Once OSDs are empty destroy them</li> </ol>"},{"location":"cluster/migrate-from-proxmox/#pve","title":"PVE","text":"<p>Once nothing is running on the node we can follow these steps to remove nodes from proxmox. </p> <p>Delete the node with:</p> <pre><code>pvecm delnode &lt;NODE&gt;\n</code></pre> <p>Then clean it out here:</p> <pre><code>root@pve01:/etc/pve/nodes# cd /etc/pve/nodes\nroot@pve01:/etc/pve/nodes# rm -R pve05/\n</code></pre> <p>You can also clean the node itself out by deleting:</p> <pre><code>systemctl stop pve-cluster corosync\npmxcfs -l\nrm /etc/corosync/*\nrm /etc/pve/corosync.conf\nkillall pmxcfs\nsystemctl start pve-cluster\n</code></pre> <p>And <code>rm -R /etc/pve/nodes</code> but I didn't get that far.</p>"},{"location":"cluster/migrate-from-proxmox/#update-ms-01-bios","title":"Update MS-01 BIOS","text":"<p>While I'm at it there's a BIOS update to apply.</p> <p>First upgrade BIOS for MS-01. - tutorial  - download</p>"},{"location":"cluster/migrate-from-proxmox/#clean-drives","title":"Clean Drives","text":"<p>Boot using Gparted and delete any data on the drives we will be using. This is especially important for Ceph as that is picky and hard to fix up the drives from Talos. </p> <p>Once you add the node to the cluster, before configurign Ceph, run this to wipe the partition table:</p> <pre><code>$ cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: disk-wipe\n  namespace: rook-ceph\nspec:\n  restartPolicy: Never\n  nodeName: talosm01\n  containers:\n  - name: disk-wipe\n    image: busybox\n    securityContext:\n      privileged: true\n    command: [\"/bin/sh\", \"-c\", \"dd if=/dev/zero bs=1M count=100 oflag=direct of=/dev/nvme0n1\"]\nEOF\npod/disk-wipe created\n\n$ kubectl wait --timeout=900s --for=jsonpath='{.status.phase}=Succeeded' pod disk-wipe\npod/disk-wipe condition met\n\n$ kubectl delete pod disk-wipe\npod \"disk-wipe\" deleted\n</code></pre>"},{"location":"cluster/pikvm-readme/","title":"PiKVM","text":"<p>See <code>./pikvm.yaml</code> for example config - I am manually keeping this in sync with what is on the device.</p>"},{"location":"cluster/pikvm-readme/#editing-config","title":"Editing Config","text":"<p>To edit config in pikvm terminal:</p> <pre><code>su -\nrw\nnano /etc/kvmd/override.yaml\nro\nexit\n</code></pre>"},{"location":"cluster/pikvm-readme/#configure-tesmart","title":"Configure TeSMART","text":"<p>TODO this was tricky document before moving</p>"},{"location":"cluster/pikvm-readme/#configure-wol","title":"Configure WOL","text":""},{"location":"cluster/pikvm-readme/#monitoring","title":"Monitoring","text":"<p>See notes here which I have copied below.</p>"},{"location":"cluster/pikvm-readme/#monitoring_1","title":"Monitoring","text":"<p>This is done ON the KVM! </p>"},{"location":"cluster/pikvm-readme/#install-node-exporter","title":"Install node-exporter","text":"<pre><code>pacman -S prometheus-node-exporter\nsystemctl enable --now prometheus-node-exporter\n</code></pre>"},{"location":"cluster/pikvm-readme/#install-promtail","title":"Install promtail","text":"<ol> <li> <p>Install promtail</p> <pre><code>pacman -S promtail\nsystemctl enable promtail\n</code></pre> </li> <li> <p>Override the promtail systemd service</p> <pre><code>mkdir -p /etc/systemd/system/promtail.service.d/\ncat &gt;/etc/systemd/system/promtail.service.d/override.conf &lt;&lt;EOL\n[Service]\nType=simple\nExecStart=\nExecStart=/usr/bin/promtail -config.file /etc/loki/promtail.yaml\nEOL\n</code></pre> </li> <li> <p>Add or replace the file <code>/etc/loki/promtail.yaml</code></p> <pre><code>server:\n  log_level: info\n  disable: true\n\nclient:\n  url: \"https://loki.devbu.io/loki/api/v1/push\"\n\npositions:\n  filename: /tmp/positions.yaml\n\nscrape_configs:\n  - job_name: journal\n    journal:\n      path: /run/log/journal\n      max_age: 12h\n      labels:\n        job: systemd-journal\n    relabel_configs:\n      - source_labels: [\"__journal__systemd_unit\"]\n        target_label: unit\n      - source_labels: [\"__journal__hostname\"]\n        target_label: hostname\n</code></pre> </li> <li> <p>Start promtail</p> <pre><code>systemctl daemon-reload\nsystemctl enable --now promtail.service\n</code></pre> </li> </ol>"},{"location":"cluster/rook-ceph/","title":"Rook Ceph on Omni","text":""},{"location":"cluster/rook-ceph/#todo","title":"TODO","text":"<p>Check out command here that uses MODEL to look up disk instead of /dev/nvme which has been changing around for this cluster.</p>"},{"location":"cluster/rook-ceph/#wtf","title":"WTF","text":"<p>Tip</p> <ul> <li>External Secrets (for dashboard)</li> </ul> <p>A cluster insn't a cluster with out some ceph action and that's where this rook guide on the talos docs comes in. </p> What does this do? <p>This seems cool but we will come back to it later.</p> <p>=== \"One thing\"</p> <pre><code>This is one thing to check out.\n</code></pre> <p>=== \"A second thing\"</p> <pre><code>This is a second thing to check out.\n</code></pre> <p>Back to ceph! </p>"},{"location":"cluster/rook-ceph/#fstrim","title":"fstrim","text":"<p>See Bernd-home-ops, has fstrim service runing</p> <pre><code>fstrim is a command-line utility in Linux used to discard (or \"trim\") unused blocks on a mounted filesystem. When files are deleted or moved, the filesystem updates its metadata to mark those blocks as free. However, the underlying storage device may not be aware that these blocks are no longer in use. Running fstrim informs the storage device about the unused blocks, allowing it to manage its storage space more efficiently.\n</code></pre>"},{"location":"cluster/wishlist/","title":"Wishlist","text":"<ul> <li>Glance</li> <li>Paperless</li> <li>hajimari as seen in action here</li> </ul> <pre><code>          hajimari.io/enable: \"true\"\n          hajimari.io/icon: cib:visual-studio-code\n          hajimari.io/group: \"home automation\"\n          hajimari.io/appName: \"ESPHome Configuration\"\n          hajimari.io/instance: \"admin\"\n</code></pre> <p>OR</p> <ul> <li>homepage   See https://github.com/brunnels/talos-cluster/blob/main/kubernetes/apps/automation/esphome/app/helmrelease.yaml#L75C11-L75C26</li> </ul>"},{"location":"home-automation/","title":"Home Automation","text":"<p>These documents are going to focus more on getting services up an running and less about what hardware and automations I'm using. That shall be documented elsewhere.</p> <p>For now this will be a placeholder / TODO list...</p> <ul> <li>Flash Tubesz Z-Wave PoE Kit with firmware zw-kit</li> <li>Flash TubesZB EFR32 MGM24 PoE Coordinator 2024 with firmware efr32-MGM24</li> </ul>"},{"location":"home-automation/#flash-zigbee-efr32-coordinatior","title":"Flash Zigbee EFR32 Coordinatior","text":"<p>I believe there are two pieces to the firmware - one is the silicon labs coordinator while the other is the ESP32 firmware on the board running it.</p>"},{"location":"home-automation/#silicon-labs-for-zigbee","title":"Silicon Labs for Zigbee","text":"<ol> <li>Install HAOS Addon (while we stil can) on the VM instance</li> <li>Use IP/Port for device -&gt; 192.168.50.162:6638</li> <li>Click usb device even though it's not usb</li> <li>Grab the link to the 'raw' view of the file in gitlab, in this case https://github.com/tube0013/tube_gateways/raw/refs/heads/main/models/current/tubeszb-efr32-MGM24/firmware/mgm24/ncp/4.4.3/tubesZB-EFR32-MGM24_NCP_7.4.3.gbl - the link the the file itself is just a webpage</li> <li>Use default baudrate </li> <li>Verbose mode becasue why not</li> <li>Save</li> <li>Accept the restart</li> <li>Watch logs</li> </ol> <p>Looks fine but we won't know until we get Z2M back alive:</p> <pre><code>[21:43:11] INFO: universal-silabs-flasher-up script exited with code 0\ns6-rc: info: service universal-silabs-flasher successfully started\ns6-rc: info: service legacy-services: starting\ns6-rc: info: service legacy-services successfully started\ns6-rc: info: service legacy-services: stopping\ns6-rc: info: service legacy-services successfully stopped\ns6-rc: info: service universal-silabs-flasher: stopping\ns6-rc: info: service universal-silabs-flasher successfully stopped\ns6-rc: info: service banner: stopping\ns6-rc: info: service banner successfully stopped\ns6-rc: info: service legacy-cont-init: stopping\ns6-rc: info: service legacy-cont-init successfully stopped\ns6-rc: info: service fix-attrs: stopping\ns6-rc: info: service fix-attrs successfully stopped\ns6-rc: info: service s6rc-oneshot-runner: stopping\ns6-rc: info: service s6rc-oneshot-runner successfully stopped\n</code></pre>"},{"location":"home-automation/#esphome-for-zigbee","title":"ESPHome for Zigbee","text":"<p>Hostname tubeszb-zigbee01.local</p> <p>Looks ready to go on <code>tubeszb-zigbee01.haynesnetwork:6053</code> though 6638 is the port for zigbee so not sure what 6638 is for. Has a wabsite at <code>http://tubeszb-zigbee01.haynesnetwork/</code></p>"},{"location":"home-automation/#flash-z-wave-kit","title":"Flash Z-Wave Kit","text":""},{"location":"home-automation/#zooz-radeo-firmware","title":"Zooz Radeo Firmware","text":"<p>ZWave JS UI is complainign that it want's Z-Wave SDK 7.22.1 or greater so we better do this one!</p> <p>Get the file from Zooz, at the time I did so it was here and called <code>https://www.getzooz.com/firmware/ZAC93_SDK_7.22.1_US-LR_V01R50.zip</code>. Note that I have model number <code>ZAC93</code> which is how I found this file.</p> <p>Extract the zip and verify it's a .gbl file. Then go to the device in ZWave JS UI -&gt; Advanced -&gt; Firmware update OTW and select the file.</p> <p>Note that the release notes stopped being updated at 1.20 but here in October we got 1.50</p>"},{"location":"home-automation/#esphome-for-z-wave","title":"ESPHome for Z Wave","text":"<p>Hostname tubeszb-zwave01.local</p> <p>This looks different and the firmware files look esphome related. I will use esphome to update it!</p> <p>ESPHome discovered both of these guys. After clicking Adop it just rocked and rolled. Some encryption key was spat out and I had the option to update it.</p> <p>Well, update upataded, and it seemed to point at the repo I was going to go to anyway. Now it has a website! <code>http://tubeszb-zwave01.haynesnetwork</code> and <code>tubeszb-zwave01.haynesnetwork:6638</code> seems ready to go... Guess I'll delete it for now and see how setting other stuff up goes.</p>"},{"location":"home-automation/backlog/","title":"Home Automation Backlog","text":""},{"location":"home-automation/backlog/#hass","title":"HASS","text":"<ul> <li>Setup Recorder</li> </ul>"},{"location":"home-automation/voice-assist/","title":"Voice Assistant","text":"<p>TODO </p> <ul> <li>https://github.com/acon96/home-llm</li> </ul>"},{"location":"observability/","title":"Observability","text":"<p>Do this FIRST to save time later (said noone)</p>"},{"location":"observability/#todo","title":"TODO","text":"<ul> <li>Setup kromogo</li> <li>Use kromogo on github readme after I migrate the cluster and use the other public IP</li> <li>review smartctl-exporter</li> <li>review snmp-exporter</li> <li>review unpoller</li> <li>Gatus templates need fixing</li> <li>I did not include or turned off out a bunch of stuff since the observability stack wasn't set up. I need to review resources to see what needs to be enabled</li> <li>Grafana and initial dashboards</li> <li>add custom dashboards etc</li> </ul>"},{"location":"observability/#dashboards-todo","title":"Dashboards TODO","text":"<ul> <li>Traefik Official</li> </ul>"},{"location":"observability/namespace-alerts/","title":"Namespace Alerts","text":"<p>I keep deleting stuff like this from <code>rook-ceph</code>'s namespace:</p> <pre><code>---\n# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/notification.toolkit.fluxcd.io/provider_v1beta3.json\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Provider\nmetadata:\n  name: alert-manager\n  namespace: rook-ceph\nspec:\n  type: alertmanager\n  address: http://alertmanager-operated.observability.svc.cluster.local:9093/api/v2/alerts/\n---\n# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/notification.toolkit.fluxcd.io/alert_v1beta3.json\napiVersion: notification.toolkit.fluxcd.io/v1beta3\nkind: Alert\nmetadata:\n  name: alert-manager\n  namespace: rook-ceph\nspec:\n  providerRef:\n    name: alert-manager\n  eventSeverity: error\n  eventSources:\n    - kind: HelmRelease\n      name: \"*\"\n  exclusionList:\n    - \"error.*lookup github\\\\.com\"\n    - \"error.*lookup raw\\\\.githubusercontent\\\\.com\"\n    - \"dial.*tcp.*timeout\"\n    - \"waiting.*socket\"\n  suspend: false\n</code></pre> <p>Need to get alerts going right after barebones smart home. </p>"},{"location":"observability/snmp-monitoring/","title":"SNMP Monitoring","text":"<p>I am skipping snmp_exporter for now as I do not have any devices setup to produce data it can scrape. However, I have things I can setup later:</p> <ul> <li>For Unraid we need to follow a setup guide we have some extra setup required.</li> <li>Unifi also needs a custom config to get it ready.</li> <li>APC Ups is configured here and we can do this once we get the rack one at the new place.</li> </ul>"},{"location":"observability/snmp-monitoring/#unifi","title":"Unifi","text":"<p>We may want to skip snmp for unifi and go straight to unpoller which I'll also put on the back burner for now.</p>"},{"location":"secrets/","title":"Sectets","text":"<p>This </p>"},{"location":"standardization/","title":"Index","text":""},{"location":"standardization/#standardization-roadmap-haynes-ops","title":"Standardization roadmap (haynes-ops)","text":"<p>This folder is the planning + runbook hub for modernizing <code>haynes-ops</code> toward the conventions used by the broader home-ops community, using two concrete references:</p> <ul> <li>The reference repo included in this workspace: <code>example-ops/onedr0p-home-ops</code></li> <li>The widely-forked template many home-ops repos are based on: <code>onedr0p/cluster-template</code></li> </ul> <p>The goal is to make it easier to:</p> <ul> <li>Lift patterns directly from kubesearch examples</li> <li>Reduce YAML duplication (use components + global defaults)</li> <li>Manage risk during reconciles (batching, rollback paths, edge-first where needed)</li> </ul>"},{"location":"standardization/#principles","title":"Principles","text":"<ul> <li>Docs first: we do not refactor manifests until the docs/runbooks are agreed.</li> <li>GitOps strictly: changes happen through commits; Flux applies them.</li> <li>Blast radius control: change one dimension at a time (structure vs behavior vs versions).</li> <li>Edge as proving ground: if the change is risky or hard to roll back, validate on <code>edge</code> first.</li> <li>Assume immutable fields exist: Deployments/StatefulSets can require delete/recreate when selectors/labels change.</li> </ul>"},{"location":"standardization/#current-repo-realities-important-constraints","title":"Current repo realities (important constraints)","text":"<ul> <li>Two clusters: <code>kubernetes/main</code> and <code>kubernetes/edge</code>.</li> <li>Flux entry points:</li> <li><code>kubernetes/*/flux/config/cluster.yaml</code> (GitRepository + <code>cluster</code> Kustomization)</li> <li><code>kubernetes/*/flux/apps.yaml</code> (the <code>cluster-apps</code> Kustomization that points at <code>kubernetes/*/apps</code>)</li> <li><code>kubernetes/*/flux/repositories/kustomization.yaml</code> (applies <code>kubernetes/shared/repositories</code>)</li> <li>Shared resources live under <code>kubernetes/shared/</code> (repositories, components, etc.).</li> </ul>"},{"location":"standardization/#target-direction-adapted-not-copied-blindly","title":"Target direction (adapted, not copied blindly)","text":"<p>We\u2019re aiming for:</p> <ul> <li>Components-first (Kustomize Components for repeatable patterns like volsync, alerts, gatus)</li> <li>Fewer \u201ctemplates\u201d (avoid duplicated YAML that drifts)</li> <li>Chart sources standardized (clear, consistent <code>HelmRelease</code> chart sourcing)</li> <li>Global defaults applied by Flux root Kustomization patches (where safe), similar to the patterns in the reference repo</li> </ul> <p>Important nuance (about the reference patterns):</p> <ul> <li><code>example-ops/onedr0p-home-ops</code> uses per-app <code>OCIRepository</code> objects and strong global patching defaults.</li> <li><code>haynes-ops</code> currently uses shared repositories under <code>kubernetes/shared/repositories/</code>.</li> </ul> <p>Neither is \u201cthe one true way\u201d for all home-ops repos; we\u2019re choosing what fits this repo best while reducing operational risk.</p>"},{"location":"standardization/#work-phases-ordered","title":"Work phases (ordered)","text":""},{"location":"standardization/#phase-0-documentation-set-now","title":"Phase 0: Documentation set (now)","text":"<p>Definition of done:</p> <ul> <li>This README explains the ordering, risk, and how to run/verify each phase.</li> <li>Each risky/tedious phase has a breakout runbook.</li> </ul>"},{"location":"standardization/#phase-1-edge-stabilization-prereq","title":"Phase 1: Edge stabilization (prereq)","text":"<p>We want <code>edge</code> to be a reliable proving ground before we take on risky refactors.</p> <ul> <li>Runbook: <code>edge-stabilization.md</code></li> </ul>"},{"location":"standardization/#phase-2-quick-wins-low-risk-high-value","title":"Phase 2: Quick wins (low risk, high value)","text":"<p>These should be mostly additive or purely structural:</p> <ul> <li>Remove/merge obvious duplication between <code>kubernetes/shared/templates/</code> and <code>kubernetes/shared/components/</code> where it doesn\u2019t change output.</li> <li> <p>Adopt a consistent component usage approach and document it.</p> </li> <li> <p>Runbook: <code>components-over-templates.md</code></p> </li> </ul>"},{"location":"standardization/#phase-3-standardize-chart-sourcing-medium-risk","title":"Phase 3: Standardize chart sourcing (medium risk)","text":"<p>Standardize <code>HelmRelease</code> chart sourcing patterns, starting with the <code>app-template</code> fleet, with careful batching and known recovery procedures.</p> <ul> <li>Runbook: <code>helmrelease-chartref-migration.md</code></li> <li>Related decision: <code>repository-source-strategy.md</code></li> </ul>"},{"location":"standardization/#phase-4-flux-global-defaults-patches-medium-to-high-risk","title":"Phase 4: Flux global defaults / patches (medium to high risk)","text":"<p>Expand Flux root Kustomization patching to reduce boilerplate and make remediation behavior consistent. Some defaults can materially change reconcile behavior, so we stage this carefully.</p> <ul> <li>Runbook: <code>flux-global-patches.md</code></li> </ul>"},{"location":"standardization/#phase-45-flux-operator-migration-very-high-risk-edge-first","title":"Phase 4.5: Flux Operator migration (very high risk, edge first)","text":"<p>Migrating to Flux Operator changes how Flux itself is installed and configured. Keep this as a separate project with a dedicated rollout and rollback plan.</p> <ul> <li>Runbook: <code>flux-operator-migration.md</code></li> </ul>"},{"location":"standardization/#phase-5-network-modernization-high-risk","title":"Phase 5: Network modernization (high risk)","text":"<p>Ingress \u2192 Gateway API (Traefik provider changes + resource type migrations) can cause downtime if done incorrectly. Keep this separate from other refactors.</p> <ul> <li>Existing notes (to be split later): <code>todo-refactor.md</code></li> </ul>"},{"location":"standardization/#standard-commands-copypaste","title":"Standard commands (copy/paste)","text":""},{"location":"standardization/#flux-status","title":"Flux status","text":"<pre><code>flux check\nflux get ks -A\nflux get hr -A\n</code></pre>"},{"location":"standardization/#reconcile-a-specific-resource","title":"Reconcile a specific resource","text":"<pre><code>flux reconcile helmrelease &lt;name&gt; -n &lt;namespace&gt; --with-source\nflux reconcile kustomization &lt;name&gt; -n flux-system --with-source\n</code></pre>"},{"location":"standardization/#immutable-selector-remediation-pattern","title":"Immutable selector remediation (pattern)","text":"<p>When you see <code>spec.selector ... field is immutable</code>:</p> <pre><code>flux suspend helmrelease &lt;name&gt; -n &lt;namespace&gt;\nkubectl -n &lt;namespace&gt; get deploy,sts,ds,cronjob -l helm.toolkit.fluxcd.io/name=&lt;name&gt;\nkubectl -n &lt;namespace&gt; delete deployment|statefulset|daemonset &lt;workload-name&gt; --wait=true\nflux resume helmrelease &lt;name&gt; -n &lt;namespace&gt;\nflux reconcile helmrelease &lt;name&gt; -n &lt;namespace&gt; --with-source\n</code></pre>"},{"location":"standardization/#incident-note-comfyui-rollback-during-app-template-v3-v4","title":"Incident note: <code>comfyui</code> rollback during <code>app-template</code> v3 \u2192 v4","text":"<p>During the <code>chartRef</code> migration / <code>app-template</code> v3\u2192v4 upgrade, <code>HelmRelease/ai/comfyui</code> failed and rolled back even though the Flux <code>Kustomization</code> looked \u201cclean\u201d at a glance.</p> <ul> <li>Why it failed: Kubernetes forbids changes to most <code>StatefulSet.spec</code> fields. The chart upgrade attempted a forbidden <code>StatefulSet</code> change, so Helm failed the upgrade and rolled back to <code>app-template@3.7.3</code>.</li> <li>Why KS didn\u2019t obviously show it: <code>comfyui</code> is applied with <code>wait: false</code>, so the KS primarily reflects \u201capplied manifests\u201d, not \u201cHelm upgrade succeeded\u201d. The HelmRelease status is the source of truth for chart upgrade outcomes.</li> <li>Remediation pattern: suspend HR \u2192 delete the blocking workload (StatefulSet for <code>comfyui</code>, Deployment for <code>ollama-*</code>) \u2192 resume + reconcile HR, then reconcile KS to refresh its health.</li> </ul>"},{"location":"standardization/#breakout-documents-index","title":"Breakout documents (index)","text":"<ul> <li><code>edge-stabilization.md</code>: get <code>edge</code> to a trustworthy baseline</li> <li><code>components-over-templates.md</code>: converge on components; stop template drift</li> <li><code>sops-scope-and-kustomization-namespacing.md</code>: keep SOPS confined to <code>flux/vars</code> while allowing app <code>Kustomization</code>s outside <code>flux-system</code></li> <li><code>seed-secrets-and-removing-sops.md</code>: future task \u2014 external-secrets seed strategy + removing per-app <code>*.sops.yaml</code></li> <li><code>helmrelease-chartref-migration.md</code>: migrate <code>HelmRelease</code> to <code>chartRef</code> safely (batching + recovery)</li> <li><code>flux-global-patches.md</code>: staged approach to onedr0p-style global defaults</li> <li><code>flux-operator-migration.md</code>: edge-first migration plan to Flux Operator + Flux Instance</li> <li><code>health-signals-with-wait-false.md</code>: how the reference repo gets strong health signals without relying on KS <code>wait: true</code></li> <li><code>gatus-deployment-alignment.md</code>: align Gatus deployment + substitution behavior to the reference repo</li> <li><code>repository-source-strategy.md</code>: decide shared vs per-app OCI sources, and how that affects migrations</li> <li><code>todo-refactor.md</code>: backlog (includes Gateway API migration ideas; treat as high risk)</li> </ul>"},{"location":"standardization/components-over-templates/","title":"Components over templates","text":""},{"location":"standardization/components-over-templates/#components-over-templates","title":"Components over templates","text":"<p>Your repo currently has both:</p> <ul> <li><code>kubernetes/shared/templates/</code> (copied YAML patterns)</li> <li><code>kubernetes/shared/components/</code> (Kustomize Components + reusable overlays)</li> </ul> <p>The goal is to converge on components as the primary reuse mechanism so we don\u2019t maintain two drifting sources of truth.</p>"},{"location":"standardization/components-over-templates/#why-components","title":"Why components","text":"<ul> <li>DRY and consistent: one implementation of a pattern (volsync/gatus/alerts/etc.).</li> <li>Safer upgrades: changing a component is still risky, but it is at least centralized and reviewable.</li> <li>Closer to the reference: <code>example-ops/onedr0p-home-ops</code> uses <code>kubernetes/components/...</code> and references them from app <code>ks.yaml</code>.</li> </ul>"},{"location":"standardization/components-over-templates/#current-state-inventory-haynes-ops","title":"Current state inventory (haynes-ops)","text":""},{"location":"standardization/components-over-templates/#templates-present","title":"Templates present","text":"<ul> <li><code>kubernetes/shared/templates/gatus/</code></li> <li><code>external/</code></li> <li><code>gaurded/</code> (note spelling; likely meant <code>guarded</code>)</li> <li><code>kubernetes/shared/templates/volsync/</code></li> <li><code>app-pvc/</code></li> <li><code>extra-pvc/</code></li> </ul>"},{"location":"standardization/components-over-templates/#components-present","title":"Components present","text":"<ul> <li><code>kubernetes/shared/components/common/</code></li> <li><code>alerts/</code> (alertmanager + github-status)</li> <li><code>repos/</code> (includes <code>app-template</code> OCIRepository)</li> <li><code>sops/</code></li> <li><code>kubernetes/shared/components/gatus/</code></li> <li><code>external/</code></li> <li><code>gaurded/</code> (same spelling issue)</li> <li><code>kubernetes/shared/components/volsync/aws/</code></li> </ul> <p>Observation:</p> <ul> <li>Many templates appear to have a \u201ccomponent equivalent\u201d already (<code>gatus/*</code>, <code>volsync/*</code>).</li> <li>This is a quick win area: choose one, delete the other later (after confirming no output change).</li> </ul>"},{"location":"standardization/components-over-templates/#target-state","title":"Target state","text":"<ul> <li>Treat <code>kubernetes/shared/components/**</code> as the only reuse mechanism.</li> <li>Phase out <code>kubernetes/shared/templates/**</code> once all consumers are migrated.</li> </ul>"},{"location":"standardization/components-over-templates/#migration-approach-safe-and-efficient","title":"Migration approach (safe and efficient)","text":""},{"location":"standardization/components-over-templates/#step-1-identify-consumers","title":"Step 1: Identify consumers","text":"<p>For each template directory, search for references in kustomizations.</p> <p>Examples:</p> <pre><code>rg -n \"shared/templates/gatus|shared/templates/volsync\" kubernetes\n</code></pre> <p>Record:</p> <ul> <li>which cluster(s): <code>main</code>, <code>edge</code>, or <code>shared</code></li> <li>which apps/namespaces reference it</li> <li>whether the consumer is a Flux <code>Kustomization</code> (<code>ks.yaml</code>) or a Kustomize <code>kustomization.yaml</code></li> </ul>"},{"location":"standardization/components-over-templates/#step-2-map-template-component","title":"Step 2: Map template \u2192 component","text":"<p>Create a simple mapping table during migration (keep it in this doc as you work):</p> Template Component Notes <code>shared/templates/gatus/external</code> <code>shared/components/gatus/external</code> Expected to be 1:1 <code>shared/templates/gatus/gaurded</code> <code>shared/components/gatus/gaurded</code> Spelling fix is a separate change <code>shared/templates/volsync/app-pvc</code> <code>shared/components/volsync/aws</code> May not be 1:1; inspect values/claims <code>shared/templates/volsync/extra-pvc</code> <code>shared/components/volsync/aws</code> May require additional mounts/claims"},{"location":"standardization/components-over-templates/#step-3-migrate-one-consumer-at-a-time-edge-first-if-unsure","title":"Step 3: Migrate one consumer at a time (edge first if unsure)","text":"<p>Rules:</p> <ul> <li>Do not mix \u201crefactor structure\u201d with \u201cbehavior change\u201d in the same commit.</li> <li>Keep diffs small enough that a revert is safe.</li> </ul>"},{"location":"standardization/components-over-templates/#step-4-verify-rendered-output-is-effectively-unchanged","title":"Step 4: Verify rendered output is effectively unchanged","text":"<p>Suggested verification on the cluster:</p> <ul> <li>reconcile and check health:</li> </ul> <pre><code>flux reconcile kustomization &lt;name&gt; -n flux-system --with-source\nflux get ks -A\nflux get hr -A\n</code></pre> <ul> <li>inspect the affected resources (labels/kinds/names) before and after</li> </ul> <p>If the change does alter output, document it explicitly here as a deliberate behavior change.</p>"},{"location":"standardization/components-over-templates/#step-5-remove-templates","title":"Step 5: Remove templates","text":"<p>Only after all consumers are migrated and verified:</p> <ul> <li>remove <code>kubernetes/shared/templates/*</code></li> <li>keep components as the single source of truth</li> </ul>"},{"location":"standardization/components-over-templates/#gotchas-to-avoid","title":"\u201cGotchas\u201d to avoid","text":"<ul> <li>Component vs plain kustomization: Kustomize Components are <code>apiVersion: kustomize.config.k8s.io/v1alpha1</code> and <code>kind: Component</code>. Consumers may need <code>components:</code> instead of <code>resources:</code> depending on how you wire them in.</li> <li>Path depth: when referencing components from app <code>ks.yaml</code>, use stable relative paths. Copy the reference pattern where possible.</li> <li>Spelling: <code>gaurded</code> exists in both templates and components. Renaming it to <code>guarded</code> is good hygiene but should be staged as a separate, mechanical refactor (to avoid breaking references).</li> </ul>"},{"location":"standardization/edge-stabilization/","title":"Edge stabilization","text":""},{"location":"standardization/edge-stabilization/#edge-stabilization-make-edge-a-safe-proving-ground","title":"Edge stabilization (make <code>edge</code> a safe proving ground)","text":"<p>This runbook is about getting <code>kubernetes/edge</code> to a state where we can safely trial changes before rolling them into <code>main</code>.</p>"},{"location":"standardization/edge-stabilization/#definition-of-done","title":"Definition of done","text":"<ul> <li><code>flux check</code> is clean on <code>edge</code>.</li> <li>All Flux sources and kustomizations are Ready.</li> <li>Core baseline apps are healthy (CNI/DNS/metrics/reloader at minimum).</li> <li>You can reconcile any one app and understand failures quickly (events/logs workflow).</li> </ul>"},{"location":"standardization/edge-stabilization/#quick-context-how-edge-is-wired","title":"Quick context: how <code>edge</code> is wired","text":"<p>Repo paths of interest:</p> <ul> <li>Flux config and root Kustomizations:</li> <li><code>kubernetes/edge/flux/config/cluster.yaml</code> (GitRepository + <code>cluster</code> Kustomization)</li> <li><code>kubernetes/edge/flux/apps.yaml</code> (<code>cluster-apps</code> Kustomization \u2192 <code>./kubernetes/edge/apps</code>)</li> <li><code>kubernetes/edge/flux/repositories/kustomization.yaml</code> (applies <code>kubernetes/shared/repositories</code>)</li> </ul> <p>Baseline apps currently in <code>edge</code> (not exhaustive):</p> <ul> <li><code>kubernetes/edge/apps/kube-system/</code> (cilium, coredns, metrics-server, reloader, etc.)</li> <li><code>kubernetes/edge/apps/flux-system/addons/</code> (notifications/webhooks/monitoring)</li> <li><code>kubernetes/edge/apps/observability/prometheus-operator-crds/</code></li> </ul>"},{"location":"standardization/edge-stabilization/#baseline-verification-commands","title":"Baseline verification commands","text":"<p>Run these on your workstation against the <code>edge</code> kubecontext.</p>"},{"location":"standardization/edge-stabilization/#flux-health","title":"Flux health","text":"<pre><code>flux check\nflux get sources all -A\nflux get ks -A\nflux get hr -A\n</code></pre> <p>If something is failing, pull the details:</p> <pre><code>flux get ks -A --status-selector ready=false\nflux get hr -A --status-selector ready=false\n</code></pre>"},{"location":"standardization/edge-stabilization/#cluster-baseline-pods","title":"Cluster baseline pods","text":"<pre><code>kubectl get nodes\nkubectl get pods -A --field-selector=status.phase!=Running\nkubectl -n flux-system get pods\nkubectl -n kube-system get pods\n</code></pre>"},{"location":"standardization/edge-stabilization/#events-last-50","title":"Events (last ~50)","text":"<pre><code>kubectl get events -A --sort-by=.lastTimestamp | tail -n 50\n</code></pre>"},{"location":"standardization/edge-stabilization/#reconcile-workflow-standard-approach","title":"Reconcile workflow (standard approach)","text":""},{"location":"standardization/edge-stabilization/#reconcile-the-cluster-entry-points","title":"Reconcile the cluster \u201centry points\u201d","text":"<p>Start at the top and work downward:</p> <pre><code>flux reconcile kustomization cluster -n flux-system --with-source\nflux reconcile kustomization cluster-apps -n flux-system --with-source\n</code></pre> <p>Then reconcile repositories if needed:</p> <pre><code>flux reconcile kustomization repositories -n flux-system --with-source\n</code></pre>"},{"location":"standardization/edge-stabilization/#reconcile-one-app","title":"Reconcile one app","text":"<pre><code>flux reconcile kustomization &lt;app-ks-name&gt; -n flux-system --with-source\n</code></pre> <p>Or if it is a HelmRelease-driven app:</p> <pre><code>flux reconcile helmrelease &lt;hr-name&gt; -n &lt;namespace&gt; --with-source\n</code></pre>"},{"location":"standardization/edge-stabilization/#known-failure-classes-and-what-to-do","title":"Known failure classes (and what to do)","text":""},{"location":"standardization/edge-stabilization/#1-source-artifact-errors","title":"1) Source / artifact errors","text":"<p>Symptoms: - GitRepository not ready - HelmRepository / OCIRepository not ready</p> <p>What to do: - Check <code>flux get sources -A</code> output - Verify namespaces on namespaced sources (e.g., <code>OCIRepository</code> must have <code>metadata.namespace</code>)</p>"},{"location":"standardization/edge-stabilization/#2-kustomization-build-errors","title":"2) Kustomization build errors","text":"<p>Symptoms: - \u201caccumulating resources\u201d errors - missing files / bad patches</p> <p>What to do: - <code>flux logs --kind Kustomization --name &lt;name&gt; -n flux-system</code> - inspect the referenced <code>path:</code> in the Kustomization</p>"},{"location":"standardization/edge-stabilization/#3-helm-upgrade-failures-rollbacks","title":"3) Helm upgrade failures / rollbacks","text":"<p>Symptoms: - HelmRelease stuck, repeated rollbacks - \u201cfield is immutable\u201d errors</p> <p>What to do: - <code>kubectl -n &lt;ns&gt; describe helmrelease &lt;name&gt;</code> - <code>kubectl -n &lt;ns&gt; get events --sort-by=.lastTimestamp | tail -n 50</code></p>"},{"location":"standardization/edge-stabilization/#immutable-selector-remediation-deploymentstatefulsetdaemonset","title":"Immutable selector remediation (Deployment/StatefulSet/DaemonSet)","text":"<p>If you see <code>spec.selector ... field is immutable</code>, the controller must be deleted and recreated.</p> <pre><code>flux suspend helmrelease &lt;name&gt; -n &lt;namespace&gt;\n\nkubectl -n &lt;namespace&gt; get deploy,sts,ds,cronjob -l helm.toolkit.fluxcd.io/name=&lt;name&gt;\nkubectl -n &lt;namespace&gt; delete deployment|statefulset|daemonset &lt;workload-name&gt; --wait=true\n\nflux resume helmrelease &lt;name&gt; -n &lt;namespace&gt;\nflux reconcile helmrelease &lt;name&gt; -n &lt;namespace&gt; --with-source\n</code></pre> <p>Notes: - Deleting the controller does not delete PVCs unless you delete PVCs separately.</p>"},{"location":"standardization/edge-stabilization/#edge-is-fixed-checklist-copypaste","title":"\u201cEdge is fixed\u201d checklist (copy/paste)","text":"<ul> <li>[ ] <code>flux check</code> passes</li> <li>[ ] <code>flux get ks -A</code> shows no <code>Ready=False</code></li> <li>[ ] <code>flux get hr -A</code> shows no <code>Ready=False</code></li> <li>[ ] <code>kubectl -n flux-system get pods</code> all Running/Ready</li> <li>[ ] <code>kubectl -n kube-system get pods</code> all Running/Ready (cilium + coredns healthy)</li> <li>[ ] You can reconcile one app end-to-end without guessing (events/logs workflow works)</li> </ul>"},{"location":"standardization/flux-global-patches/","title":"Flux global patches","text":""},{"location":"standardization/flux-global-patches/#flux-global-patches-defaults","title":"Flux global patches / defaults","text":"<p>This doc explains how to reduce per-app boilerplate by applying global defaults at the root Flux Kustomization layer, and how to do it safely.</p>"},{"location":"standardization/flux-global-patches/#why-this-is-worth-doing","title":"Why this is worth doing","text":"<ul> <li>Historically, this repo applied shared SOPS + substitutions via patches in:</li> <li><code>kubernetes/main/flux/apps.yaml</code></li> <li><code>kubernetes/edge/flux/apps.yaml</code></li> <li>The reference repo (<code>example-ops/onedr0p-home-ops</code>) takes this further by also injecting HelmRelease remediation defaults via patches in:</li> <li><code>d:/labspace/example-ops/onedr0p-home-ops/kubernetes/flux/cluster/ks.yaml</code></li> </ul> <p>The outcome is fewer repeated fields across <code>ks.yaml</code> and <code>helmrelease.yaml</code>, and more consistent remediation behavior.</p>"},{"location":"standardization/flux-global-patches/#current-state-haynes-ops","title":"Current state (haynes-ops)","text":"<p><code>haynes-ops</code> is moving toward the reference repo\u2019s \u201cexplicit per-app settings\u201d model:</p> <ul> <li><code>cluster-apps</code> no longer injects <code>postBuild.substituteFrom</code> by default</li> <li>SOPS decryption is configured explicitly on the few Flux <code>Kustomization</code> CRs that actually apply <code>*.sops.yaml</code></li> </ul>"},{"location":"standardization/flux-global-patches/#reference-pattern-example-opsonedr0p-home-ops","title":"Reference pattern (example-ops/onedr0p-home-ops)","text":"<p>The reference repo applies a patch to all child Kustomizations that itself contains a patch targeting HelmReleases, to inject default remediation and CRD handling.</p> <p>Key ideas (not copy/paste exact, but the pattern is useful):</p> <ul> <li>HelmRelease defaults like:</li> <li><code>install.crds: CreateReplace</code></li> <li><code>upgrade.crds: CreateReplace</code></li> <li>aggressive remediation strategies</li> <li>rollback cleanup/recreate</li> </ul> <p>This is powerful, but it changes how upgrades behave and must be staged carefully.</p>"},{"location":"standardization/flux-global-patches/#staged-adoption-plan-recommended","title":"Staged adoption plan (recommended)","text":""},{"location":"standardization/flux-global-patches/#stage-a-safe-additive-defaults-low-risk","title":"Stage A: Safe, additive defaults (low risk)","text":"<p>Goal: reduce noise without changing upgrade semantics.</p> <p>Candidate defaults to inject:</p> <ul> <li>Standardize <code>spec.interval</code> for Kustomizations (already mostly done).</li> <li>Add consistent <code>wait: false</code> or <code>wait: true</code> policy (only if you\u2019re confident).</li> <li>Add a standard <code>timeout</code> (again, only if you\u2019re confident).</li> </ul> <p>Rollout:</p> <ul> <li>Start on <code>edge</code>.</li> <li>Add patch logic to <code>kubernetes/edge/flux/apps.yaml</code>.</li> <li>Reconcile <code>cluster-apps</code> and ensure no child Kustomizations become NotReady.</li> </ul>"},{"location":"standardization/flux-global-patches/#stage-b-helmrelease-remediation-defaults-medium-to-high-risk","title":"Stage B: HelmRelease remediation defaults (medium to high risk)","text":"<p>Goal: make Helm behavior consistent, especially around failures and CRDs.</p> <p>Candidate defaults (examples from the reference pattern):</p> <ul> <li>CRD upgrade/install:</li> <li><code>spec.install.crds: CreateReplace</code></li> <li><code>spec.upgrade.crds: CreateReplace</code></li> <li>Consistent rollback/upgrade remediation:</li> <li><code>cleanupOnFail: true</code></li> <li><code>remediation.retries</code> bounds</li> </ul> <p>Why it is risky:</p> <ul> <li>CRD replace behavior can be disruptive if a chart manages CRDs unexpectedly.</li> <li>\u201cRecreate on rollback/upgrade\u201d can cause extra restarts.</li> </ul> <p>Rollout:</p> <ul> <li>Apply to <code>edge</code> first.</li> <li>Keep the patch scoped via <code>target</code> selectors (labels or name patterns) if possible.</li> <li>Consider opt-out labels (e.g., allow a HelmRelease to disable defaults).</li> </ul>"},{"location":"standardization/flux-global-patches/#stage-c-structural-simplification-optional","title":"Stage C: Structural simplification (optional)","text":"<p>Once defaults are stable:</p> <ul> <li>remove redundant blocks from individual <code>ks.yaml</code> (but only when verified)</li> </ul>"},{"location":"standardization/flux-global-patches/#suggested-implementation-technique","title":"Suggested implementation technique","text":"<p>In <code>kubernetes/*/flux/apps.yaml</code> (root <code>cluster-apps</code> Kustomization), add a patch targeting child Kustomizations, and inside it include a <code>spec.patches</code> entry targeting HelmReleases.</p> <p>Key safety tactics:</p> <ul> <li>Prefer opt-in selectors for higher-risk behavior changes (safer rollout), or opt-out selectors for low-risk defaults.</li> <li>Batch and reconcile: add defaults, reconcile, and check readiness before continuing.</li> </ul>"},{"location":"standardization/flux-global-patches/#operational-commands","title":"Operational commands","text":""},{"location":"standardization/flux-global-patches/#reconcile-root","title":"Reconcile root","text":"<pre><code>flux reconcile kustomization cluster -n flux-system --with-source\nflux reconcile kustomization cluster-apps -n flux-system --with-source\n</code></pre>"},{"location":"standardization/flux-global-patches/#verify-nothing-regressed","title":"Verify nothing regressed","text":"<pre><code>flux get ks -A\nflux get hr -A\nkubectl get events -A --sort-by=.lastTimestamp | tail -n 50\n</code></pre>"},{"location":"standardization/flux-global-patches/#exit-criteria-definition-of-done","title":"Exit criteria (definition of done)","text":"<ul> <li>Root patches applied on <code>edge</code> without introducing new NotReady resources.</li> <li>Defaults documented (what we set, why, and how to opt out).</li> <li>Only then consider rolling the same patches to <code>main</code>.</li> </ul>"},{"location":"standardization/flux-kustomization-namespace-moves-pvc-safety/","title":"Flux kustomization namespace moves pvc safety","text":""},{"location":"standardization/flux-kustomization-namespace-moves-pvc-safety/#flux-kustomization-namespace-moves-pvc-safety","title":"Flux Kustomization namespace moves &amp; PVC safety","text":"<p>Moving a Flux <code>Kustomization</code> CR (e.g. from <code>flux-system</code> to an app namespace) is not a \u201crename\u201d. In Kubernetes it is delete + create (namespace is immutable), and Flux may prune resources that were previously applied by the old object.</p> <p>This matters most for PersistentVolumeClaims (PVCs) and other stateful resources.</p>"},{"location":"standardization/flux-kustomization-namespace-moves-pvc-safety/#what-happened-the-gotcha","title":"What happened (the gotcha)","text":"<ul> <li>A Flux <code>Kustomization</code> maintains an inventory of the objects it applied.</li> <li>When a <code>Kustomization</code> is deleted, Flux\u2019s finalizer can garbage-collect (prune) objects from that inventory.</li> <li>If the inventory included PVCs, they can be deleted as part of the move.</li> </ul>"},{"location":"standardization/flux-kustomization-namespace-moves-pvc-safety/#symptoms","title":"Symptoms","text":"<ul> <li>Pods become <code>Pending</code> with events like:</li> <li><code>persistentvolumeclaim \"&lt;name&gt;\" not found</code></li> <li><code>pod has unbound immediate PersistentVolumeClaims</code></li> <li>Flux <code>Kustomization</code> health checks time out waiting for PVCs.</li> </ul>"},{"location":"standardization/flux-kustomization-namespace-moves-pvc-safety/#make-pvcs-sticky-recommended-default","title":"Make PVCs \u201csticky\u201d (recommended default)","text":"<p>For any PVC you never want Flux to delete automatically, set:</p> <pre><code>metadata:\n  annotations:\n    kustomize.toolkit.fluxcd.io/prune: disabled\n</code></pre> <p>Notes: - This annotation is respected by Flux when pruning resources (including inventory-based pruning). - For Helm-managed PVCs, the analogous pattern is often <code>helm.sh/resource-policy: keep</code> (Helm behavior, not Flux).</p>"},{"location":"standardization/flux-kustomization-namespace-moves-pvc-safety/#safer-namespace-move-procedure-runbook","title":"Safer namespace-move procedure (runbook)","text":"<p>Do this before moving the <code>Kustomization</code> CR:</p> <ol> <li>Inventory the stateful objects under the Kustomization path</li> <li>PVCs (<code>kind: PersistentVolumeClaim</code>)</li> <li>PV-backed StatefulSets</li> <li> <p>VolSync restored PVCs / ReplicationDestination resources</p> </li> <li> <p>Protect PVCs</p> </li> <li> <p>Add <code>kustomize.toolkit.fluxcd.io/prune: disabled</code> to the PVC manifests (or PVC templates) that must never be deleted.</p> </li> <li> <p>Optional: disable pruning for the old Kustomization (belt-and-suspenders)</p> </li> <li>Temporarily set <code>spec.prune: false</code> on the old <code>Kustomization</code>, reconcile it, then perform the move.</li> <li> <p>This reduces the chance of deletions during the cutover if the old Kustomization is removed.</p> </li> <li> <p>Move the Kustomization</p> </li> <li>Change <code>metadata.namespace</code> to the target namespace.</li> <li> <p>Ensure <code>spec.sourceRef.namespace</code> and any <code>dependsOn[].namespace</code> are explicit.</p> </li> <li> <p>Force a reconciliation</p> </li> </ol> <pre><code>flux reconcile kustomization &lt;name&gt; -n &lt;new-namespace&gt; --with-source\nflux reconcile helmrelease &lt;name&gt; -n &lt;new-namespace&gt; --with-source\n</code></pre>"},{"location":"standardization/flux-kustomization-namespace-moves-pvc-safety/#recovery-if-you-already-got-bit","title":"Recovery if you already got bit","text":"<p>If the Kustomization path still contains the PVC manifests (and they are not Helm-owned), the fastest recovery is usually:</p> <pre><code>flux reconcile kustomization &lt;name&gt; -n &lt;namespace&gt; --with-source\nkubectl -n &lt;namespace&gt; get pvc\n</code></pre> <p>If the PVCs were deleted and not defined in Git (or were Helm-owned), you must restore them via the correct owner (Git manifests, Helm values, or storage restore process).</p>"},{"location":"standardization/flux-operator-migration/","title":"Flux operator migration","text":""},{"location":"standardization/flux-operator-migration/#flux-operator-migration-reference-aligned-edge-first","title":"Flux Operator migration (reference-aligned, edge-first)","text":"<p>This doc explains how we would migrate <code>haynes-ops</code> from \u201cFlux installed via flux2 install manifests (GOTK style)\u201d to \u201cFlux managed by Flux Operator + Flux Instance\u201d, similar to <code>example-ops/onedr0p-home-ops</code>.</p> <p>This is a separate project from chartRef/component/global-patches work because it changes how Flux itself is installed and configured.</p>"},{"location":"standardization/flux-operator-migration/#tldr","title":"TL;DR","text":"<ul> <li>You do not need Flux Operator to do the current standardization work.</li> <li>If you adopt it, do it edge-first with a clean rollback plan.</li> <li>You can keep SOPS, External Secrets, or both. The reference repo uses External Secrets heavily.</li> </ul>"},{"location":"standardization/flux-operator-migration/#current-state-haynes-ops","title":"Current state (haynes-ops)","text":"<p>Flux components are installed by a non-Flux-tracked bootstrap kustomization:</p> <ul> <li><code>kubernetes/main/bootstrap/flux/kustomization.yaml</code></li> <li><code>kubernetes/edge/bootstrap/flux/kustomization.yaml</code></li> </ul> <p>Both pull:</p> <ul> <li><code>github.com/fluxcd/flux2/manifests/install?...</code></li> </ul> <p>Then Flux is configured via the normal Flux CRDs under:</p> <ul> <li><code>kubernetes/*/flux/config/*</code></li> <li><code>kubernetes/*/flux/apps.yaml</code></li> </ul>"},{"location":"standardization/flux-operator-migration/#reference-state-example-opsonedr0p-home-ops","title":"Reference state (example-ops/onedr0p-home-ops)","text":"<p>The reference repo installs Flux as two apps in <code>flux-system</code>:</p> <ul> <li><code>flux-operator</code>: installs the operator itself</li> <li><code>flux-instance</code>: defines the Flux instance (which controllers, sync settings, controller patches, etc.)</li> </ul> <p>Key examples:</p> <ul> <li><code>kubernetes/apps/flux-system/flux-operator/app/helmrelease.yaml</code></li> <li><code>kubernetes/apps/flux-system/flux-instance/app/helmrelease.yaml</code></li> </ul> <p>Notable: the instance HelmRelease includes configuration such as:</p> <ul> <li>Which controllers are enabled (source/kustomize/helm/notification)</li> <li>The Git sync URL/ref/path</li> <li>Patches that tune controllers (workers, memory limits, caching, feature gates)</li> </ul>"},{"location":"standardization/flux-operator-migration/#secrets-management-note-external-secrets-vs-sops","title":"Secrets management note (External Secrets vs SOPS)","text":""},{"location":"standardization/flux-operator-migration/#what-the-reference-repo-does","title":"What the reference repo does","text":"<p>The reference repo uses External Secrets to populate Flux-related secrets (example: GitHub webhook token):</p> <ul> <li><code>kubernetes/apps/flux-system/flux-instance/app/externalsecret.yaml</code></li> </ul> <p>That ExternalSecret reads from a <code>ClusterSecretStore</code> (OnePassword Connect in the reference repo):</p> <ul> <li><code>kubernetes/apps/external-secrets/onepassword/app/clustersecretstore.yaml</code></li> </ul>"},{"location":"standardization/flux-operator-migration/#what-haynes-ops-does-today","title":"What haynes-ops does today","text":"<p>You already have both patterns in this repo:</p> <ul> <li>SOPS for <code>flux-system</code> \u201ccluster vars\u201d:</li> <li><code>kubernetes/main/flux/vars/cluster-secrets.sops.yaml</code></li> <li>External Secrets used by many apps (including Flux notifications):</li> <li>e.g. <code>kubernetes/main/apps/flux-system/addons/app/notifications/github/externalsecret.yaml</code></li> </ul> <p>So the migration question is not \u201cSOPS vs External Secrets\u201d. It\u2019s \u201cwhat secrets does Flux Operator/Instance need, and how do we source them safely\u201d.</p>"},{"location":"standardization/flux-operator-migration/#migration-goals","title":"Migration goals","text":"<p>Pick the minimal set of goals up front:</p> <ul> <li>Manage Flux controller deployment via Flux Operator (instead of bootstrapped GOTK install manifests)</li> <li>Keep the same Git source of truth and sync paths</li> <li>Preserve existing Flux objects and app reconciliation behavior as much as possible</li> </ul>"},{"location":"standardization/flux-operator-migration/#high-risk-areas-plan-for-them","title":"High-risk areas (plan for them)","text":"<ul> <li>Control plane of GitOps changes: you are changing the system that applies everything else.</li> <li>Double-managing controllers: if GOTK controllers and operator-managed controllers overlap, you can end up with conflicting deployments/CRDs.</li> <li>CRDs: operator/instance distribution will install CRDs; you must avoid multiple conflicting CRD managers.</li> <li>Sync \u201cpath\u201d differences: reference repo syncs <code>kubernetes/flux/cluster</code>; you sync <code>kubernetes/&lt;cluster&gt;/flux</code> and <code>kubernetes/&lt;cluster&gt;/apps</code> via different Kustomizations.</li> </ul>"},{"location":"standardization/flux-operator-migration/#edge-first-migration-outline-recommended","title":"Edge-first migration outline (recommended)","text":""},{"location":"standardization/flux-operator-migration/#phase-0-decide-the-end-state-wiring","title":"Phase 0: Decide the end-state wiring","text":"<p>Answer these (document the decisions in this doc as you proceed):</p> <ul> <li>Where should the \u201cinstance sync path\u201d point for edge?</li> <li>likely <code>kubernetes/edge/flux/config</code> or a new dedicated path that only defines flux config</li> <li>Do we keep <code>cluster</code> and <code>cluster-apps</code> Kustomizations as-is?</li> <li>recommended: yes, initially</li> <li>Do we keep SOPS for cluster vars?</li> <li>recommended: yes (add ExternalSecrets only where needed)</li> </ul>"},{"location":"standardization/flux-operator-migration/#phase-1-ensure-external-secrets-baseline-exists-on-edge-if-required","title":"Phase 1: Ensure External Secrets baseline exists on edge (if required)","text":"<p>If you want Flux webhook tokens or operator credentials sourced dynamically, ensure <code>external-secrets</code> is installed and a <code>ClusterSecretStore</code> exists on edge.</p>"},{"location":"standardization/flux-operator-migration/#phase-2-install-flux-operator-on-edge-as-an-app","title":"Phase 2: Install Flux Operator on edge (as an app)","text":"<p>Mirror the reference repo structure in edge (names and locations can differ, but keep the concept):</p> <ul> <li>Add <code>flux-operator</code> app to <code>kubernetes/edge/apps/flux-system/</code> using:</li> <li>an <code>OCIRepository</code> for the chart</li> <li>a <code>HelmRelease</code> to install it</li> </ul>"},{"location":"standardization/flux-operator-migration/#phase-3-install-flux-instance-on-edge","title":"Phase 3: Install Flux Instance on edge","text":"<p>Add <code>flux-instance</code> app that declares:</p> <ul> <li>instance distribution artifact version</li> <li>enabled components</li> <li>sync settings (repo URL/ref/path)</li> <li>controller tuning patches</li> </ul>"},{"location":"standardization/flux-operator-migration/#phase-4-prevent-controller-conflict","title":"Phase 4: Prevent controller conflict","text":"<p>Before turning off GOTK-managed controllers, confirm:</p> <ul> <li>operator-managed controllers are running and healthy</li> <li>they are reconciling correctly (sources/ks/hr)</li> </ul> <p>Then disable/remove the old controller deployments installed by the bootstrap method.</p> <p>This step is the highest risk. Do it on edge and only once you have a rollback path (git revert and/or re-apply bootstrap install).</p>"},{"location":"standardization/flux-operator-migration/#phase-5-roll-forward-to-main","title":"Phase 5: Roll forward to main","text":"<p>After edge is stable for a while:</p> <ul> <li>repeat the process on main, with a maintenance window mindset</li> </ul>"},{"location":"standardization/flux-operator-migration/#verification-checklist-edge","title":"Verification checklist (edge)","text":"<p>After each phase:</p> <pre><code>flux check\nflux get sources all -A\nflux get ks -A\nflux get hr -A\nkubectl -n flux-system get pods\nkubectl get events -A --sort-by=.lastTimestamp | tail -n 50\n</code></pre>"},{"location":"standardization/flux-operator-migration/#rollback-strategy-edge","title":"Rollback strategy (edge)","text":"<p>Rollback should be \u201csimple and fast\u201d:</p> <ul> <li>revert the commit that introduced operator/instance</li> <li>re-apply the bootstrap Flux install manifests if controllers were removed</li> </ul> <p>Do not attempt a partial rollback that leaves two controller sets alive.</p>"},{"location":"standardization/flux-operator-migration/#recommended-sequencing-with-our-other-standardization-work","title":"Recommended sequencing with our other standardization work","text":"<p>Do not attempt Flux Operator migration while you are simultaneously doing:</p> <ul> <li>large <code>chartRef</code> migrations</li> <li>major global patch behavior changes</li> <li>ingress/gateway migrations</li> </ul> <p>Treat Flux Operator migration as its own project, after edge is stabilized.</p>"},{"location":"standardization/gatus-deployment-alignment/","title":"Gatus deployment alignment","text":""},{"location":"standardization/gatus-deployment-alignment/#gatus-deployment-alignment-matching-example-opsonedr0p-home-ops","title":"Gatus deployment alignment (matching <code>example-ops/onedr0p-home-ops</code>)","text":"<p>This repo historically deployed Gatus using:</p> <ul> <li>A <code>HelmRelease</code> based on <code>app-template</code> v3</li> <li>A sidecar (<code>k8s-sidecar</code>) to watch <code>gatus.io/enabled</code> ConfigMaps across namespaces</li> <li>Postgres (<code>cloudnative-pg</code>) + init container (<code>postgres-init</code>)</li> <li>Flux global substitution/SOPS injection (repo-wide default behavior)</li> </ul> <p>The reference repo (<code>example-ops/onedr0p-home-ops</code>) has moved to a simpler pattern:</p> <ul> <li>SQLite on a local PVC</li> <li><code>gatus-sidecar</code> to auto-discover endpoints from Kubernetes resources</li> <li>Per-app <code>OCIRepository</code> + <code>HelmRelease.spec.chartRef</code></li> <li>Explicitly disabling Flux substitution for the generated configmap so <code>${VAR}</code> remains for Gatus runtime env var expansion</li> </ul>"},{"location":"standardization/gatus-deployment-alignment/#why-this-matters-for-substitution-patterns","title":"Why this matters for \u201csubstitution patterns\u201d","text":"<p>Gatus config supports <code>${ENV_VAR}</code> expansion at runtime.</p> <p>In <code>haynes-ops</code>, Flux currently injects substitution defaults globally, which means <code>${VAR}</code> inside generated ConfigMaps can be interpreted by Flux during build time. The reference repo avoids this by adding:</p> <pre><code>generatorOptions:\n  annotations:\n    kustomize.toolkit.fluxcd.io/substitute: disabled\n</code></pre> <p>We now do the same for Gatus configmaps.</p>"},{"location":"standardization/gatus-deployment-alignment/#target-pattern-what-were-aligning-to","title":"Target pattern (what we\u2019re aligning to)","text":"<ul> <li>Storage: SQLite (no Postgres dependency)</li> <li>Controller: StatefulSet + <code>volumeClaimTemplates</code> for <code>/config</code></li> <li>Endpoint discovery: <code>gatus-sidecar</code> init container</li> <li>Chart sourcing: <code>HelmRelease.spec.chartRef</code> -&gt; per-app <code>OCIRepository</code> (named <code>gatus</code>)</li> </ul>"},{"location":"standardization/gatus-deployment-alignment/#operational-implications-gotchas","title":"Operational implications / gotchas","text":"<ul> <li>PVC naming changes: with <code>volumeClaimTemplates</code> the created PVC name is derived from the template + StatefulSet name (e.g. <code>config-gatus-0</code>), not a stable <code>gatus</code> PVC name.</li> <li>This does not prevent VolSync, but it does make generic \u201c<code>${APP}</code>-named claim\u201d templates harder.</li> <li>Chart upgrade risk: switching from <code>app-template</code> v3 to v4 (via <code>chartRef</code> to an <code>OCIRepository</code>) can trigger immutable-field issues for Deployments.</li> <li>For Gatus we change controller type to StatefulSet anyway; plan for a delete/recreate of old workloads if Helm can\u2019t mutate cleanly.</li> <li>Old PVC cleanup: if a previous <code>gatus</code> PVC exists (from the \u201cexistingClaim\u201d pattern), it may become unused after the switch and should be cleaned up deliberately once you\u2019re satisfied with the new deployment.</li> </ul>"},{"location":"standardization/gatus-deployment-alignment/#rollout-steps-main-cluster","title":"Rollout steps (main cluster)","text":"<ol> <li>Ensure the new manifests are committed (GitOps source of truth).</li> <li>Reconcile the app:</li> </ol> <pre><code>flux reconcile kustomization gatus -n flux-system --with-source\nflux reconcile helmrelease gatus -n observability --with-source\n</code></pre> <ol> <li>If Helm gets stuck and the old workload blocks the new controller type:</li> <li>Suspend the HelmRelease, delete the old Deployment/StatefulSet, resume and reconcile (same pattern as the <code>app-template</code> v3 -&gt; v4 remediation).</li> </ol>"},{"location":"standardization/gatus-deployment-alignment/#follow-ups-optional","title":"Follow-ups (optional)","text":"<ul> <li>Decide whether we still want the legacy \u201clabelled ConfigMap endpoint injection\u201d (<code>gatus.io/enabled</code>) approach anywhere.</li> <li>If the <code>gatus-sidecar</code> approach is sufficient, we can phase out <code>kubernetes/shared/templates/gatus/*</code> over time.</li> </ul>"},{"location":"standardization/health-signals-with-wait-false/","title":"Health signals with wait false","text":""},{"location":"standardization/health-signals-with-wait-false/#health-signals-with-wait-false-reference-repo-patterns","title":"Health signals with <code>wait: false</code> (reference repo patterns)","text":"<p>This documents the patterns used in the reference repo (<code>example-ops/onedr0p-home-ops</code>) to get strong health signals even when many Flux <code>Kustomization</code>s are set to <code>wait: false</code>.</p> <p>The goal is not to copy blindly, but to capture the \u201csignal sources\u201d we may want to adopt in <code>haynes-ops</code> later.</p>"},{"location":"standardization/health-signals-with-wait-false/#the-core-idea","title":"The core idea","text":"<p><code>wait: false</code> prevents a Flux <code>Kustomization</code> from blocking on health checks for everything it applies. That reduces \u201cdeadlocks\u201d (one broken app shouldn\u2019t stall the whole cluster-apps tree), but it means you need other mechanisms to surface failures quickly and reliably.</p> <p>The reference repo gets those signals from:</p> <ul> <li>Flux Notifications (events \u2192 Alertmanager / GitHub status)</li> <li>Prometheus scraping + alert rules for Flux itself</li> <li>HelmRelease remediation defaults (retries/rollback behavior is consistent)</li> <li>Targeted <code>healthChecks</code> / <code>healthCheckExprs</code> for a small set of \u201ccritical infra\u201d Kustomizations</li> </ul>"},{"location":"standardization/health-signals-with-wait-false/#pattern-1-flux-notifications-errors-alertmanager","title":"Pattern 1: Flux Notifications (errors \u2192 Alertmanager)","text":"<p>Reference repo uses Flux notifications to send error events for most Flux objects to Alertmanager.</p> <p>Files:</p> <ul> <li><code>example-ops/onedr0p-home-ops/kubernetes/components/alerts/alertmanager/provider.yaml</code></li> <li><code>example-ops/onedr0p-home-ops/kubernetes/components/alerts/alertmanager/alert.yaml</code></li> </ul> <p>Notable behaviors:</p> <ul> <li>Broad coverage: the <code>Alert</code> watches errors from <code>FluxInstance</code>, <code>GitRepository</code>, <code>Kustomization</code>, <code>HelmRelease</code>, <code>OCIRepository</code>, etc.</li> <li>Noise control: an <code>exclusionList</code> drops known-flaky patterns (e.g., transient GitHub DNS/timeout lookups).</li> </ul> <p>Why it matters with <code>wait: false</code>:</p> <ul> <li>Even if a <code>Kustomization</code> \u201capplies clean\u201d, a failing <code>HelmRelease</code> still emits events and flips <code>READY=False</code>. Notifications catch that without relying on KS health gating.</li> </ul>"},{"location":"standardization/health-signals-with-wait-false/#pattern-2-flux-notifications-kustomization-github-status","title":"Pattern 2: Flux Notifications (Kustomization \u2192 GitHub status)","text":"<p>Reference repo also drives GitHub status from Kustomizations.</p> <p>Files:</p> <ul> <li><code>example-ops/onedr0p-home-ops/kubernetes/components/alerts/github-status/provider.yaml</code></li> <li><code>example-ops/onedr0p-home-ops/kubernetes/components/alerts/github-status/alert.yaml</code></li> </ul> <p>Why it matters:</p> <ul> <li>This is a \u201cdeveloper feedback loop\u201d signal. It\u2019s not cluster correctness on its own, but it makes breakage visible even when <code>wait: false</code> is used widely.</li> </ul>"},{"location":"standardization/health-signals-with-wait-false/#pattern-3-flux-instance-monitoring-podmonitor-prometheusrule","title":"Pattern 3: Flux Instance monitoring (PodMonitor + PrometheusRule)","text":"<p>The reference repo monitors Flux controllers and alerts if Flux itself isn\u2019t healthy.</p> <p>Files:</p> <ul> <li><code>example-ops/onedr0p-home-ops/kubernetes/apps/flux-system/flux-instance/app/podmonitor.yaml</code></li> <li><code>example-ops/onedr0p-home-ops/kubernetes/apps/flux-system/flux-instance/app/prometheusrule.yaml</code></li> <li><code>example-ops/onedr0p-home-ops/kubernetes/apps/flux-system/flux-instance/app/grafanadashboard.yaml</code></li> </ul> <p>What it does:</p> <ul> <li>PodMonitor scrapes metrics from:</li> <li><code>source-controller</code></li> <li><code>kustomize-controller</code></li> <li><code>helm-controller</code></li> <li><code>notification-controller</code></li> <li>PrometheusRule alerts on:</li> <li>FluxInstanceAbsent (no metrics)</li> <li>FluxInstanceNotReady (ready != True for 5m)</li> <li>GrafanaDashboard imports Flux dashboards from upstream URLs (Flux Operator + flux2 monitoring example).</li> </ul> <p>Why it matters with <code>wait: false</code>:</p> <ul> <li>If Flux controllers are degraded, <code>wait</code> settings on downstream Kustomizations are irrelevant. This gives a first-line signal that \u201cGitOps is broken\u201d.</li> </ul>"},{"location":"standardization/health-signals-with-wait-false/#pattern-4-fast-apply-strong-remediation-defaults","title":"Pattern 4: \u201cFast apply\u201d + \u201cstrong remediation\u201d defaults","text":"<p>Reference repo uses a root <code>cluster-apps</code> Kustomization with <code>wait: false</code>, but patches all child Kustomizations to inject HelmRelease remediation defaults.</p> <p>File:</p> <ul> <li><code>example-ops/onedr0p-home-ops/kubernetes/flux/cluster/ks.yaml</code></li> </ul> <p>What it injects (high level):</p> <ul> <li>Install/upgrade strategies (<code>RetryOnFailure</code>, <code>RemediateOnFailure</code>)</li> <li>CRD install/upgrade behavior (<code>CreateReplace</code>)</li> <li>Rollback cleanup + recreate</li> <li>Upgrade retries/remediation behavior</li> </ul> <p>Why it matters with <code>wait: false</code>:</p> <ul> <li>You get consistent \u201cself-healing\u201d without relying on Kustomization health gating.</li> <li>Failures become visible primarily in <code>HelmRelease</code> status/events, and are forwarded via notifications.</li> </ul>"},{"location":"standardization/health-signals-with-wait-false/#pattern-5-targeted-healthchecks-healthcheckexprs-selected-infra","title":"Pattern 5: Targeted <code>healthChecks</code> / <code>healthCheckExprs</code> (selected infra)","text":"<p>The reference repo adds explicit <code>healthChecks</code> and <code>healthCheckExprs</code> on some infrastructure Kustomizations (examples include <code>cert-manager</code>, <code>onepassword</code>, and <code>cloudflare-dns</code>).</p> <p>Examples:</p> <ul> <li><code>example-ops/onedr0p-home-ops/kubernetes/apps/cert-manager/cert-manager/ks.yaml</code></li> <li>health check a <code>HelmRelease</code> plus a <code>ClusterIssuer</code></li> <li>expression-based readiness for <code>ClusterIssuer</code> conditions</li> <li><code>example-ops/onedr0p-home-ops/kubernetes/apps/external-secrets/onepassword/ks.yaml</code></li> <li>expression-based readiness for <code>ClusterSecretStore</code> conditions</li> <li><code>example-ops/onedr0p-home-ops/kubernetes/apps/network/cloudflare-dns/ks.yaml</code></li> <li>health check a <code>HelmRelease</code> plus a CRD</li> </ul> <p>Important nuance:</p> <ul> <li><code>healthChecks</code> are only useful if Flux is actually performing health evaluation for that Kustomization. In <code>haynes-ops</code>, if we adopt this pattern, we should be explicit about which Kustomizations are \u201cgated\u201d (set <code>wait: true</code> where we truly want readiness to reflect downstream health) vs \u201cfast apply\u201d (keep <code>wait: false</code>).</li> </ul>"},{"location":"standardization/health-signals-with-wait-false/#receiver-pattern-github-webhook-triggers-immediate-reconcile","title":"Receiver pattern: GitHub webhook triggers immediate reconcile","text":"<p>The reference repo creates a <code>Receiver</code> that listens for GitHub push events and triggers reconciliation of the <code>GitRepository</code> and root <code>Kustomization</code>.</p> <p>File:</p> <ul> <li><code>example-ops/onedr0p-home-ops/kubernetes/apps/flux-system/flux-instance/app/receiver.yaml</code></li> </ul> <p>Why it matters:</p> <ul> <li>Reduces time-to-detect for broken commits without requiring very tight intervals.</li> </ul>"},{"location":"standardization/health-signals-with-wait-false/#what-we-could-adopt-in-haynes-ops-later","title":"What we could adopt in <code>haynes-ops</code> (later)","text":"<p>If we want \u201cstrong health signals\u201d while keeping <code>wait: false</code> broadly:</p> <ul> <li>Flux \u2192 Alertmanager notifications: likely the highest value and lowest risk.</li> <li>Flux controller metrics scraping + rules: also high value; requires Prometheus stack integration.</li> <li>HelmRelease remediation defaults via root patches: medium risk (behavior change), but powerful.</li> <li>Selective <code>wait: true</code> + healthChecks on critical infra: use sparingly to avoid deadlocks.</li> <li>GitHub status provider/alert: optional; most useful if you care about PR/commit status feedback.</li> </ul>"},{"location":"standardization/helmrelease-chartref-migration/","title":"HelmRelease <code>chartRef</code> Migration Plan (app-template)","text":""},{"location":"standardization/helmrelease-chartref-migration/#goal","title":"Goal","text":"<p>Standardize all <code>HelmRelease</code> resources that currently use:</p> <pre><code>  chart:\n    spec:\n      chart: app-template\n      version: 3.7.3\n      interval: 30m\n      sourceRef:\n        kind: HelmRepository\n        name: bjw-s\n        namespace: flux-system\n</code></pre> <p>\u2026to instead use:</p> <pre><code>  chartRef:\n    kind: OCIRepository\n    name: app-template\n    namespace: flux-system\n</code></pre> <p>This change is a reference mechanism migration (from <code>chart.spec</code> to <code>chartRef</code>). It can still trigger workload replacement if the chart version/labels differ from what originally created the workload.</p>"},{"location":"standardization/helmrelease-chartref-migration/#why-this-matters","title":"Why this matters","text":"<ul> <li>Consistency: <code>chartRef</code> is simpler and aligns with the rest of the repo (example: <code>ollama-assist01</code>).</li> <li>Shared source of truth: <code>OCIRepository/app-template</code> in <code>flux-system</code> becomes the chart source for all app-template-based releases.</li> <li>Operational reality: upgrades can fail with <code>spec.selector ... field is immutable</code> when the chart changes workload selector labels. That requires a delete/recreate of the controller object.</li> </ul>"},{"location":"standardization/helmrelease-chartref-migration/#pre-flight-checklist","title":"Pre-flight checklist","text":"<ul> <li>Confirm the source exists:</li> <li><code>OCIRepository/app-template</code> exists in <code>flux-system</code> and is healthy.</li> <li> <p>All <code>HelmRelease</code> that will reference it include:</p> <ul> <li><code>spec.chartRef.namespace: flux-system</code></li> </ul> </li> <li> <p>Identify all targets (repo-side):</p> </li> </ul> <pre><code>rg -n --glob \"**/helmrelease.yaml\" \"chart:\\n\\\\s+spec:\\n\\\\s+chart: app-template\" kubernetes\n</code></pre>"},{"location":"standardization/helmrelease-chartref-migration/#files-in-this-repo-that-need-the-change","title":"Files in this repo that need the change","text":"<p>The following <code>HelmRelease</code> files currently use <code>spec.chart.spec.chart: app-template</code> (and in practice are pinned to <code>3.7.3</code>) and should be migrated to <code>spec.chartRef</code>:</p> <ul> <li><code>kubernetes/main/apps/ai/ollama/assist02/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/ai/ollama/prime/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/photos/immich/server/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/photos/immich/machine-learning/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/office/paperless-ngx/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/network/cloudflare-ddns/app-www/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/network/cloudflare-ddns/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/media/plex/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/home-automation/home-assistant/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/home-automation/esphome/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/database/dragonfly/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/downloads/ytdl-sub/youtube/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/downloads/ytdl-sub/peloton/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/downloads/ytdl-sub/peloton-config-manager/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/storage/storage-util/rsync-scans/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/storage/storage-util/rsync-photos/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/storage/storage-util/rsync-music/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/observability/kromgo/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/observability/gatus/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/home-automation/zwave/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/home-automation/go2rtc/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/network/multus/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/ai/wyoming-protocol/whisper/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/ai/wyoming-protocol/piper/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/ai/wyoming-protocol/openwakeword/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/ai/wyoming-protocol/speech-to-phrase/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/home-automation/zigbee2mqtt/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/external-secrets/onepassword-connect/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/home-automation/music-assistant/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/photos/hass-immich-addon/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/media/overseerr/app/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/kube-system/intel-device-plugin/exporter/helmrelease.yaml</code></li> <li><code>kubernetes/main/apps/ai/stable-diffusion/comfyui/helmrelease.yaml</code></li> </ul>"},{"location":"standardization/helmrelease-chartref-migration/#repo-changes-mechanical","title":"Repo changes (mechanical)","text":"<p>For each target <code>HelmRelease</code>:</p> <ol> <li>Remove the entire block:</li> </ol> <pre><code>spec:\n  chart:\n    spec:\n      chart: app-template\n      version: 3.7.3\n      interval: 30m\n      sourceRef:\n        kind: HelmRepository\n        name: bjw-s\n        namespace: flux-system\n</code></pre> <ol> <li>Add (or replace with) <code>chartRef</code>:</li> </ol> <pre><code>spec:\n  chartRef:\n    kind: OCIRepository\n    name: app-template\n    namespace: flux-system\n</code></pre> <p>Notes: - Keep <code>spec.interval</code> as-is. - This does not change <code>metadata.namespace</code> (the release namespace); it only changes where the chart is sourced from.</p>"},{"location":"standardization/helmrelease-chartref-migration/#rollout-strategy-recommended","title":"Rollout strategy (recommended)","text":"<p>Do this in batches to reduce blast radius and make it easy to isolate failures:</p> <ul> <li>Batch 1 (low risk): CronJobs</li> <li>Batch 2: Deployments</li> <li>Batch 3 (higher operational impact): StatefulSets</li> </ul> <p>If a batch fails due to immutable selectors, fix those releases before proceeding to the next batch.</p>"},{"location":"standardization/helmrelease-chartref-migration/#cluster-commands-per-release","title":"Cluster commands (per release)","text":""},{"location":"standardization/helmrelease-chartref-migration/#1-force-a-fresh-reconcile-normal-path","title":"1) Force a fresh reconcile (normal path)","text":"<pre><code>flux reconcile helmrelease &lt;name&gt; -n &lt;namespace&gt; --with-source\n</code></pre>"},{"location":"standardization/helmrelease-chartref-migration/#2-debug-stuck-or-repeated-rollback","title":"2) Debug \u201cstuck\u201d or repeated rollback","text":"<p>Get status and events:</p> <pre><code>flux get helmrelease -n &lt;namespace&gt; &lt;name&gt;\nkubectl -n &lt;namespace&gt; describe helmrelease &lt;name&gt;\n</code></pre> <p>Inspect what Helm thinks it is applying:</p> <pre><code>kubectl -n &lt;namespace&gt; get events --sort-by=.lastTimestamp | tail -n 50\n</code></pre>"},{"location":"standardization/helmrelease-chartref-migration/#handling-immutable-selector-failures-deploymentstatefulsetdaemonset","title":"Handling immutable selector failures (Deployment/StatefulSet/DaemonSet)","text":"<p>Symptom in helm-controller logs/events:</p> <ul> <li><code>... is invalid: spec.selector ... field is immutable</code></li> </ul> <p>This means the controller already exists and Kubernetes will not allow changing <code>.spec.selector</code>.</p>"},{"location":"standardization/helmrelease-chartref-migration/#fix-pattern-gitops-friendly","title":"Fix pattern (GitOps-friendly)","text":"<p>1) Pause reconciliation for the release:</p> <pre><code>flux suspend helmrelease &lt;name&gt; -n &lt;namespace&gt;\n</code></pre> <p>2) Find the workload objects for that release (common labels present in this repo):</p> <pre><code>kubectl -n &lt;namespace&gt; get deploy,sts,ds,cronjob -l helm.toolkit.fluxcd.io/name=&lt;name&gt;\n</code></pre> <p>3) Delete the controller object that is failing.</p> <p>Deployment:</p> <pre><code>kubectl -n &lt;namespace&gt; delete deployment &lt;workload-name&gt; --wait=true\n</code></pre> <p>StatefulSet:</p> <pre><code>kubectl -n &lt;namespace&gt; delete statefulset &lt;workload-name&gt; --wait=true\n</code></pre> <p>DaemonSet:</p> <pre><code>kubectl -n &lt;namespace&gt; delete daemonset &lt;workload-name&gt; --wait=true\n</code></pre> <p>Notes: - Deleting the controller does not delete PVCs (unless you delete PVCs separately). - If you are unsure, verify volumes are PVC-backed (<code>existingClaim</code>) before deletion.</p> <p>4) Resume and reconcile:</p> <pre><code>flux resume helmrelease &lt;name&gt; -n &lt;namespace&gt;\nflux reconcile helmrelease &lt;name&gt; -n &lt;namespace&gt; --with-source\n</code></pre> <p>5) Verify the new controller is on the intended chart version:</p> <pre><code>kubectl -n &lt;namespace&gt; get deploy &lt;workload-name&gt; -o jsonpath='{.metadata.labels.helm\\.sh/chart}{\"\\n\"}'\n</code></pre>"},{"location":"standardization/helmrelease-chartref-migration/#cronjob-notes","title":"CronJob notes","text":"<p>CronJobs typically do not hit selector immutability issues in the same way as Deployments/StatefulSets. If a CronJob upgrade fails, the simplest recovery is usually:</p> <pre><code>flux suspend helmrelease &lt;name&gt; -n &lt;namespace&gt;\nkubectl -n &lt;namespace&gt; delete cronjob &lt;workload-name&gt; --wait=true\nflux resume helmrelease &lt;name&gt; -n &lt;namespace&gt;\nflux reconcile helmrelease &lt;name&gt; -n &lt;namespace&gt; --with-source\n</code></pre>"},{"location":"standardization/helmrelease-chartref-migration/#standard-verification-steps","title":"Standard verification steps","text":"<p>After each release migration:</p> <pre><code>flux get helmrelease -n &lt;namespace&gt; &lt;name&gt;\nkubectl -n &lt;namespace&gt; get pods -l app.kubernetes.io/instance=&lt;name&gt;\nkubectl -n &lt;namespace&gt; get ingress,svc -l app.kubernetes.io/instance=&lt;name&gt;\n</code></pre>"},{"location":"standardization/helmrelease-chartref-migration/#known-gotchas-in-this-repo","title":"Known gotchas in this repo","text":"<ul> <li>Cross-namespace chart sources: if <code>chartRef.namespace</code> is omitted, you can get confusing failures. Always set:</li> <li><code>spec.chartRef.namespace: flux-system</code></li> <li>Shared source bump affects many apps: when <code>OCIRepository/app-template</code> tag is changed, multiple apps can upgrade at once. Prefer batching/PRs rather than big-bang.</li> </ul>"},{"location":"standardization/repository-source-strategy/","title":"Repository source strategy","text":""},{"location":"standardization/repository-source-strategy/#repository-source-strategy-helmrepository-vs-ocirepository-shared-vs-per-app","title":"Repository source strategy (HelmRepository vs OCIRepository, shared vs per-app)","text":"<p>This repo is mid-transition between two viable patterns for chart sourcing. This doc makes the decision explicit and ties it directly to the <code>HelmRelease.chartRef</code> migration work.</p>"},{"location":"standardization/repository-source-strategy/#the-two-patterns","title":"The two patterns","text":""},{"location":"standardization/repository-source-strategy/#pattern-a-shared-sources-current-haynes-ops-direction","title":"Pattern A: Shared sources (current haynes-ops direction)","text":"<p>Concept:</p> <ul> <li>Define chart sources centrally under <code>kubernetes/shared/repositories/</code> and/or <code>kubernetes/shared/components/common/repos/</code>.</li> <li><code>HelmRelease</code> objects reference those shared sources (cross-namespace via <code>spec.chartRef.namespace: flux-system</code>).</li> </ul> <p>What you have today:</p> <ul> <li>Many shared <code>HelmRepository</code> resources under <code>kubernetes/shared/repositories/helm/</code></li> <li>A shared <code>OCIRepository/app-template</code> under <code>kubernetes/shared/components/common/repos/app-template/ocirepository.yaml</code></li> </ul> <p>Pros:</p> <ul> <li>Less per-app YAML (one chart source for many apps).</li> <li>Central governance of versions.</li> </ul> <p>Cons:</p> <ul> <li>Blast radius: bumping the shared source version upgrades many apps at once.</li> <li>Harder to do \u201cmigrate one app at a time\u201d chart source/version strategies.</li> </ul>"},{"location":"standardization/repository-source-strategy/#pattern-b-per-app-ocirepository-reference-repo-pattern","title":"Pattern B: Per-app OCIRepository (reference repo pattern)","text":"<p>Concept:</p> <ul> <li>Each app defines its own <code>OCIRepository</code> in its <code>app/</code> directory (usually named after the app).</li> <li><code>HelmRelease.spec.chartRef</code> points at that app\u2019s OCIRepository.</li> </ul> <p>Example from <code>example-ops/onedr0p-home-ops</code>:</p> <ul> <li><code>kubernetes/apps/default/atuin/app/ocirepository.yaml</code> defines <code>OCIRepository/atuin</code> pointing at <code>app-template</code> tag <code>4.6.2</code></li> <li><code>kubernetes/apps/default/atuin/app/helmrelease.yaml</code> uses:</li> <li><code>spec.chartRef.kind: OCIRepository</code></li> <li><code>spec.chartRef.name: atuin</code></li> </ul> <p>Pros:</p> <ul> <li>Fine-grained upgrades: version bumps affect a single app.</li> <li>Easier to trial and roll back per app.</li> </ul> <p>Cons:</p> <ul> <li>More objects/YAML in the repo.</li> <li>Needs naming conventions and discipline to stay consistent.</li> </ul>"},{"location":"standardization/repository-source-strategy/#how-this-impacts-chartref-migration-app-template-fleet","title":"How this impacts <code>chartRef</code> migration (app-template fleet)","text":"<p>Your current <code>chartRef</code> migration doc (<code>helmrelease-chartref-migration.md</code>) is primarily about changing:</p> <ul> <li>from <code>spec.chart.spec</code> (HelmRepository + chart name/version)</li> <li>to <code>spec.chartRef</code> (OCIRepository reference)</li> </ul> <p>But the source strategy decision controls the risk:</p> <ul> <li>If you use shared <code>OCIRepository/app-template</code>, then changing its <code>spec.ref.tag</code> can upgrade dozens of apps.</li> <li>If you use per-app OCIRepositories, the migration can be done app-by-app with a smaller blast radius.</li> </ul>"},{"location":"standardization/repository-source-strategy/#recommended-approach-pragmatic","title":"Recommended approach (pragmatic)","text":"<p>Given your repo structure and desire for efficiency:</p> <ol> <li>Short-term: keep the shared <code>OCIRepository/app-template</code> for app-template to minimize YAML churn.</li> <li>Add guardrails:</li> <li>Document batching and recovery (already captured in <code>helmrelease-chartref-migration.md</code>).</li> <li>Avoid bumping <code>app-template</code> tag during the mechanical migration unless intentionally planned.</li> <li>Medium-term: for apps that repeatedly need special handling (or where independent upgrades are valuable), consider migrating those specific apps to per-app OCIRepository later.</li> </ol> <p>This keeps the migration work focused while leaving a path to the reference-repo pattern where it provides clear value.</p>"},{"location":"standardization/repository-source-strategy/#decision-rule-for-new-services-what-we-should-do-going-forward","title":"Decision rule for new services (what we should do going forward)","text":"<p>Use this rule of thumb to avoid rework while you\u2019re onboarding many new services:</p> <ul> <li>Prefer shared sources when:</li> <li>many apps intentionally share the same chart (e.g., <code>app-template</code>), and</li> <li> <p>you\u2019re comfortable upgrading them as a fleet (or at least accepting a larger blast radius)</p> </li> <li> <p>Prefer per-app <code>OCIRepository</code> when:</p> </li> <li>the chart is effectively \u201cowned by\u201d one app (no other app will ever reference it), or</li> <li>you want strict blast-radius control for version bumps, or</li> <li>the app/chart has special upgrade behavior and you want to stage upgrades independently</li> </ul> <p>Concrete example:</p> <ul> <li>Per-app makes sense for things like <code>kubernetes/main/apps/storage/rook-ceph/</code> where nothing else would ever need to reference the same <code>OCIRepository</code>.</li> </ul>"},{"location":"standardization/repository-source-strategy/#decision-checklist","title":"Decision checklist","text":"<p>Answer these before changing the strategy:</p> <ul> <li>Do we want one PR to upgrade app-template everywhere, or N PRs per app?</li> <li>Are we comfortable with a shared source bump triggering selector immutability events across many apps?</li> <li>Do we want Renovate to manage versions centrally or per app?</li> </ul>"},{"location":"standardization/repository-source-strategy/#operational-notes-flux-namespaces","title":"Operational notes (Flux + namespaces)","text":"<ul> <li><code>OCIRepository</code> is namespaced. If it\u2019s shared, it should live in <code>flux-system</code> and all <code>HelmRelease.spec.chartRef.namespace</code> should explicitly point there.</li> <li>If per-app OCIRepositories are used, the OCIRepository typically lives in the same namespace as the app\u2019s resources (or at least is managed in the same kustomization path). Make the namespace choice explicit and consistent.</li> </ul>"},{"location":"standardization/seed-secrets-and-removing-sops/","title":"Seed secrets and removing sops","text":""},{"location":"standardization/seed-secrets-and-removing-sops/#seed-secrets-removing-per-app-sops-future-standardization-task","title":"Seed secrets &amp; removing per-app SOPS (future standardization task)","text":"<p>This doc captures a future standardization task: converge on the \u201cseed secret\u201d bootstrap strategy used in the reference repo (<code>example-ops/onedr0p-home-ops</code>) so we can eventually remove per-app <code>*.sops.yaml</code> secrets (like <code>onepassword-connect.secret.sops.yaml</code>) while keeping GitOps bootstrapping viable.</p> <p>This is intentionally not an immediate refactor; it affects bootstrap ordering and failure modes.</p>"},{"location":"standardization/seed-secrets-and-removing-sops/#current-state-haynes-ops","title":"Current state (haynes-ops)","text":"<p>We still have some apps that apply SOPS-encrypted manifests inside the app path, e.g.:</p> <ul> <li><code>kubernetes/main/apps/external-secrets/onepassword-connect/app/onepassword-connect.secret.sops.yaml</code></li> </ul> <p>Those apps require the Flux <code>Kustomization</code> applying them to have:</p> <pre><code>spec:\n  decryption:\n    provider: sops\n    secretRef:\n      name: sops-age\n</code></pre> <p>Important: <code>spec.decryption.secretRef.name</code> is namespaced. The referenced secret must exist in the same namespace as the Flux <code>Kustomization</code> CR.</p> <p>So if a KS lives in <code>flux-system</code>, it depends on <code>flux-system/sops-age</code>.</p> <p>This is why KS namespace moves are easy for apps with no <code>*.sops.yaml</code>, and harder for apps that still apply SOPS secrets.</p>"},{"location":"standardization/seed-secrets-and-removing-sops/#reference-pattern-onedr0p-home-ops-externalsecret-seed-concept","title":"Reference pattern (onedr0p-home-ops): ExternalSecret \u201cseed\u201d concept","text":"<p>The reference repo uses <code>ExternalSecret</code> to materialize a Kubernetes Secret that\u2019s then used by a <code>ClusterSecretStore</code>:</p> <ul> <li><code>ExternalSecret/onepassword</code> creates <code>Secret/onepassword-secret</code></li> <li><code>ClusterSecretStore/onepassword</code> uses <code>Secret/onepassword-secret</code> to authenticate</li> </ul> <p>Files in the reference repo:</p> <ul> <li><code>kubernetes/apps/external-secrets/onepassword/app/externalsecret.yaml</code></li> <li><code>kubernetes/apps/external-secrets/onepassword/app/clustersecretstore.yaml</code></li> </ul>"},{"location":"standardization/seed-secrets-and-removing-sops/#the-chicken-and-egg-problem-must-be-documented","title":"The chicken-and-egg problem (must be documented)","text":"<p>At first glance, this is circular:</p> <ul> <li><code>ExternalSecret</code> needs a working <code>ClusterSecretStore</code></li> <li><code>ClusterSecretStore</code> needs credentials that are typically provided by\u2026 a Secret that the <code>ExternalSecret</code> would create</li> </ul> <p>Therefore: a new cluster needs a seed path to create the first secret(s) required to reach the external secret backend.</p>"},{"location":"standardization/seed-secrets-and-removing-sops/#seed-strategies-choose-one","title":"Seed strategies (choose one)","text":""},{"location":"standardization/seed-secrets-and-removing-sops/#strategy-a-minimal-sops-seed-recommended-for-haynes-ops-today","title":"Strategy A: Minimal SOPS \u201cseed\u201d (recommended for haynes-ops today)","text":"<p>Use SOPS only for the smallest possible set of bootstrap secrets, typically:</p> <ul> <li>1Password Connect credentials/token secret (or equivalent)</li> <li>any Flux webhook tokens needed during bootstrap (optional)</li> </ul> <p>Then everything else is sourced via External Secrets.</p> <p>This keeps GitOps bootstrapping intact and avoids \u201ckubectl create secret\u201d day-0 steps.</p> <p>What changes over time:</p> <ul> <li>We remove per-app SOPS secrets (inside app paths)</li> <li>We retain a tiny bootstrap SOPS footprint (likely in <code>flux-system</code> or a dedicated bootstrap path)</li> </ul>"},{"location":"standardization/seed-secrets-and-removing-sops/#strategy-b-manual-day-0-secret-not-preferred-for-this-repo","title":"Strategy B: Manual day-0 secret (not preferred for this repo)","text":"<p>Create the initial secret by hand (once) with <code>kubectl create secret ...</code>, then proceed with External Secrets.</p> <p>This breaks \u201cGitOps strictly\u201d for day-0, so we generally avoid it here.</p>"},{"location":"standardization/seed-secrets-and-removing-sops/#strategy-c-workload-identity-if-available","title":"Strategy C: Workload identity (if available)","text":"<p>Authenticate to the secret backend without a static secret committed anywhere (cloud IAM patterns).</p> <p>Typically not available / not worth the complexity for a homelab.</p>"},{"location":"standardization/seed-secrets-and-removing-sops/#proposed-staged-plan-haynes-ops","title":"Proposed staged plan (haynes-ops)","text":""},{"location":"standardization/seed-secrets-and-removing-sops/#phase-0-inventory-and-classify","title":"Phase 0: Inventory and classify","text":"<ul> <li>List all <code>*.sops.yaml</code> used under <code>kubernetes/main/apps/**</code></li> <li>For each, decide: bootstrap seed vs app secret</li> </ul>"},{"location":"standardization/seed-secrets-and-removing-sops/#phase-1-constrain-sops-to-seed-only","title":"Phase 1: Constrain SOPS to \u201cseed only\u201d","text":"<ul> <li>Keep SOPS assets confined to <code>flux-system</code> (already the direction of <code>sops-scope-and-kustomization-namespacing.md</code>)</li> <li>Move per-app SOPS secrets out of app paths where possible</li> </ul>"},{"location":"standardization/seed-secrets-and-removing-sops/#phase-2-convert-app-secrets-to-externalsecret","title":"Phase 2: Convert app secrets to ExternalSecret","text":"<p>Example target: replace</p> <ul> <li><code>onepassword-connect.secret.sops.yaml</code></li> </ul> <p>with:</p> <ul> <li>an <code>ExternalSecret</code> that materializes the same keys into <code>external-secrets/onepassword-connect-secret</code></li> </ul>"},{"location":"standardization/seed-secrets-and-removing-sops/#phase-3-namespace-moves-become-simple","title":"Phase 3: Namespace moves become simple","text":"<p>Once an app no longer applies <code>*.sops.yaml</code>, its Flux <code>Kustomization</code> can live in the app namespace without requiring <code>sops-age</code> in that namespace.</p>"},{"location":"standardization/seed-secrets-and-removing-sops/#operational-notes-risk","title":"Operational notes (risk)","text":"<ul> <li>Namespace moving a KS is delete+create. Inventory pruning can delete resources; PVCs must be protected (<code>kustomize.toolkit.fluxcd.io/prune: disabled</code>).</li> <li>For VolSync-populated PVCs, KS moves can also trigger restore workflows; treat those moves as potentially data-affecting even if the PVC survives.</li> </ul>"},{"location":"standardization/seed-secrets-and-removing-sops/#related-docs","title":"Related docs","text":"<ul> <li><code>sops-scope-and-kustomization-namespacing.md</code></li> <li><code>flux-operator-migration.md</code> (reference repo uses External Secrets heavily)</li> </ul>"},{"location":"standardization/sops-scope-and-kustomization-namespacing/","title":"Sops scope and kustomization namespacing","text":""},{"location":"standardization/sops-scope-and-kustomization-namespacing/#keep-sops-in-fluxvars-stop-stamping-sops-into-every-app-namespace","title":"Keep SOPS in <code>flux/vars</code> (stop stamping SOPS into every app namespace)","text":"<p>Goal: do not require <code>sops-age</code>, <code>cluster-settings</code>, or <code>cluster-secrets</code> to exist in every application namespace just because Flux <code>Kustomization</code> CRs live outside <code>flux-system</code>.</p> <p>This keeps us aligned with the \u201capp Kustomizations live in the app namespace\u201d pattern while we still bootstrap with SOPS.</p>"},{"location":"standardization/sops-scope-and-kustomization-namespacing/#why-this-is-necessary-in-this-repo","title":"Why this is necessary in this repo","text":"<p>Historically, <code>kubernetes/*/flux/apps.yaml</code> injected SOPS decryption + <code>postBuild.substituteFrom</code> into all child Flux Kustomizations by default. That caused breakage when Flux <code>Kustomization</code> CRs lived outside <code>flux-system</code> (because <code>sops-age</code>, <code>cluster-settings</code>, etc. would be expected in that app namespace).</p> <p>As part of aligning with the reference repo\u2019s \u201cexplicit per-app settings\u201d model, <code>haynes-ops</code> has moved away from default-on injection.</p>"},{"location":"standardization/sops-scope-and-kustomization-namespacing/#current-coupling-we-want-to-remove","title":"Current coupling we want to remove","text":"<p>The component <code>kubernetes/shared/components/common/sops</code> creates:</p> <ul> <li><code>Secret/sops-age</code></li> <li><code>Secret/cluster-secrets</code> (SOPS-encrypted)</li> <li><code>ConfigMap/cluster-settings</code></li> </ul> <p>Because it\u2019s a component (and those manifests have no explicit <code>metadata.namespace</code>), including it in a namespace kustomization (example: <code>kubernetes/main/apps/frontend/kustomization.yaml</code>) stamps those objects into that namespace.</p> <p>We want to stop doing that so SOPS stays confined to:</p> <ul> <li><code>kubernetes/*/flux/vars/*</code> (applied in <code>flux-system</code>)</li> </ul>"},{"location":"standardization/sops-scope-and-kustomization-namespacing/#the-pattern-what-to-do","title":"The pattern (what to do)","text":""},{"location":"standardization/sops-scope-and-kustomization-namespacing/#step-1-remove-commonsops-from-app-namespaces","title":"Step 1: Remove <code>common/sops</code> from app namespaces","text":"<ul> <li>Stop including <code>kubernetes/shared/components/common/sops</code> from namespace-level kustomizations (like <code>kubernetes/main/apps/frontend/kustomization.yaml</code>).</li> </ul>"},{"location":"standardization/sops-scope-and-kustomization-namespacing/#step-2-make-sops-decryption-explicit-only-where-needed","title":"Step 2: Make SOPS decryption explicit only where needed","text":"<p>If a Flux <code>Kustomization</code> applies any <code>*.sops.yaml</code> files, add decryption to that <code>Kustomization</code> CR:</p> <pre><code>spec:\n  decryption:\n    provider: sops\n    secretRef:\n      name: sops-age\n</code></pre> <p>If it does not apply SOPS files, do not add decryption.</p>"},{"location":"standardization/sops-scope-and-kustomization-namespacing/#step-3-keep-sourceref-explicit","title":"Step 3: Keep <code>sourceRef</code> explicit","text":"<p>If the <code>GitRepository</code> source lives in <code>flux-system</code> (as it does in this repo), then app Kustomizations in other namespaces must set:</p> <pre><code>spec:\n  sourceRef:\n    kind: GitRepository\n    name: haynes-ops\n    namespace: flux-system\n</code></pre> <p>(<code>homepage</code> and <code>omni</code> already do this.)</p>"},{"location":"standardization/sops-scope-and-kustomization-namespacing/#example-frontendhomepage","title":"Example: <code>frontend/homepage</code>","text":"<p><code>kubernetes/main/apps/frontend/homepage/ks.yaml</code> is a Flux <code>Kustomization</code> in the <code>frontend</code> namespace. It does not apply SOPS files, so it does not need <code>spec.decryption</code>. It does need an explicit <code>sourceRef.namespace: flux-system</code>.</p>"},{"location":"standardization/sops-scope-and-kustomization-namespacing/#definition-of-done-for-a-namespace","title":"Definition of done (for a namespace)","text":"<ul> <li>Namespace kustomization no longer includes <code>shared/components/common/sops</code></li> <li>Flux <code>Kustomization</code> CRs in that namespace only include <code>spec.decryption</code> when they actually apply <code>*.sops.yaml</code></li> <li>No dependency on stamping SOPS assets into app namespaces</li> </ul>"},{"location":"standardization/todo-refactor/","title":"Improvement Plan for haynes-ops","text":"<p>This document outlines a plan to modernize <code>haynes-ops</code> by adopting best practices from <code>onedr0p-home-ops</code>, focusing on stability, maintainability, and feature parity. It includes high-level goals and detailed step-by-step implementation guides.</p>"},{"location":"standardization/todo-refactor/#1-network-modernization-ingress-to-gateway-api","title":"1. Network Modernization: Ingress to Gateway API","text":"<p>Goal: Replace standard Ingress resources with the Kubernetes Gateway API to enable advanced routing and automated discovery for monitoring.</p>"},{"location":"standardization/todo-refactor/#overview","title":"Overview","text":"<ul> <li>Current State: <code>haynes-ops</code> uses Traefik with standard <code>Ingress</code> resources.</li> <li>Target State: Adopt Gateway API using <code>HTTPRoute</code> resources.</li> <li>Why:<ul> <li>Unlocks <code>auto-httproute</code> discovery for Gatus (see Monitoring section).</li> <li>Standardized configuration via <code>app-template</code>'s <code>route</code> block.</li> <li>More powerful traffic routing capabilities.</li> </ul> </li> <li>Risk: High. Changing the ingress layer can cause downtime.</li> </ul>"},{"location":"standardization/todo-refactor/#detailed-implementation-plan","title":"Detailed Implementation Plan","text":""},{"location":"standardization/todo-refactor/#phase-a-enable-gateway-api-in-traefik-edge-cluster-first","title":"Phase A: Enable Gateway API in Traefik (Edge Cluster First)","text":"<p>Prerequisite: Traefik must be running on the Edge cluster to test this safely. Add a task to deploy a minimal Traefik instance to Edge if not present.</p> <ol> <li> <p>Verify Gateway API CRDs:     Ensure the Gateway API CRDs are present on the cluster.     <pre><code>kubectl get crd gateways.gateway.networking.k8s.io\n</code></pre> Note: If missing, they must be installed via a standard installation method (e.g., bundled with a controller chart or a separate HelmRelease) before proceeding.</p> </li> <li> <p>Update Traefik HelmRelease:     Modify <code>kubernetes/main/apps/network/traefik/traefik-internal/app/helmrelease.yaml</code> to enable the Gateway provider.</p> <pre><code># ... inside values\nproviders:\n  kubernetesGateway:\n    enabled: true\n    experimentalChannel: false\n</code></pre> </li> <li> <p>Create a GatewayClass and Gateway:     Create <code>kubernetes/main/apps/network/traefik/gateway/gateway.yaml</code>:</p> <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\n  name: traefik\nspec:\n  controllerName: traefik.io/gateway-controller\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\n  name: external\n  namespace: network\nspec:\n  gatewayClassName: traefik\n  listeners:\n    - name: web\n      port: 80\n      protocol: HTTP\n      allowedRoutes:\n        namespaces:\n          from: All\n    - name: websecure\n      port: 443\n      protocol: HTTPS\n      allowedRoutes:\n        namespaces:\n          from: All\n</code></pre> </li> </ol>"},{"location":"standardization/todo-refactor/#phase-b-migrate-an-app","title":"Phase B: Migrate an App","text":"<p>Update an app's <code>HelmRelease</code> to use <code>route</code> instead of <code>ingress</code> (e.g., starting with <code>podinfo</code> or <code>whoami</code>).</p> <ul> <li> <p>From (<code>haynes-ops</code> Ingress):     <pre><code>ingress:\n  app:\n    className: traefik-external\n    hosts:\n      - host: app.example.com\n        paths:\n          - path: /\n            service:\n              identifier: app\n              port: http\n</code></pre></p> </li> <li> <p>To (<code>onedr0p</code> Route):     <pre><code>route:\n  app:\n    parentRefs:\n      - name: external\n        namespace: network\n    hostnames:\n      - app.example.com\n    rules:\n      - backendRefs:\n          - name: app\n            port: 80\n</code></pre></p> </li> </ul>"},{"location":"standardization/todo-refactor/#2-flux-modernization-oci-global-patches","title":"2. Flux Modernization: OCI &amp; Global Patches","text":"<p>Goal: Improve performance with OCI Helm charts and reduce boilerplate code using Global Patches.</p>"},{"location":"standardization/todo-refactor/#overview_1","title":"Overview","text":"<ul> <li>OCI Artifacts: Switch from <code>HelmRepository</code> to <code>OCIRepository</code> for faster, more reliable artifact delivery.</li> <li>Global Patches: Use Kustomize patches in the root <code>apps.yaml</code> to enforce defaults (like <code>decryption: provider: sops</code>) across all apps, removing the need for repetitive <code>ks.yaml</code> files.</li> <li>Flux Structure: Align with <code>onedr0p</code>'s optimized GitOps Toolkit structure.</li> </ul>"},{"location":"standardization/todo-refactor/#detailed-implementation-plan_1","title":"Detailed Implementation Plan","text":""},{"location":"standardization/todo-refactor/#21-switch-to-oci-repositories","title":"2.1. Switch to OCI Repositories","text":"<p><code>onedr0p</code> defines charts as OCI artifacts.</p> <ol> <li> <p>Define OCI Repository:     In <code>kubernetes/shared/repositories/oci-repositories.yaml</code>:     <pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: OCIRepository\nmetadata:\n  name: bjw-s-charts\n  namespace: flux-system\nspec:\n  interval: 1h\n  url: oci://ghcr.io/bjw-s/helm-charts\n  ref:\n    tag: latest\n</code></pre></p> </li> <li> <p>Update HelmReleases:     Change <code>sourceRef</code> in your apps.     <pre><code># Old\nchart:\n  spec:\n    chart: app-template\n    sourceRef:\n      kind: HelmRepository\n      name: bjw-s\n\n# New\nchart:\n  spec:\n    chart: app-template\n    sourceRef:\n      kind: OCIRepository\n      name: bjw-s-charts\n</code></pre></p> </li> </ol>"},{"location":"standardization/todo-refactor/#22-reduce-ksyaml-boilerplate-global-patches","title":"2.2. Reduce <code>ks.yaml</code> Boilerplate (Global Patches)","text":"<p>Instead of defining <code>decryption: provider: sops</code> in every single <code>ks.yaml</code>, inject it globally.</p> <ol> <li> <p>Edit <code>kubernetes/main/flux/apps.yaml</code>:     Add a patch to the root <code>cluster-apps</code> Kustomization.</p> <pre><code>patches:\n  - patch: |-\n      apiVersion: kustomize.toolkit.fluxcd.io/v1\n      kind: Kustomization\n      metadata:\n        name: not-used\n      spec:\n        decryption:\n          provider: sops\n          secretRef:\n            name: sops-age\n    target:\n      kind: Kustomization\n      group: kustomize.toolkit.fluxcd.io\n</code></pre> </li> </ol>"},{"location":"standardization/todo-refactor/#23-flux-kustomization-namespace-migration","title":"2.3. Flux Kustomization Namespace Migration","text":"<p>Goal: Move app <code>Kustomization</code> resources out of <code>flux-system</code> and into a dedicated namespace (e.g., <code>flux-apps</code> or <code>flux-kustomizations</code>) to improve organization and RBAC separation.</p> <ul> <li>Why: <code>onedr0p</code> and others do this to avoid polluting the system namespace.</li> <li>Challenge: <code>flux-system</code> is special; moving things out requires explicit RBAC for the Kustomize controller.</li> <li>Plan:<ol> <li>Create Namespace <code>flux-apps</code>.</li> <li>Update <code>kubernetes/main/flux/apps.yaml</code> (the root) to target this namespace for child Kustomizations.</li> <li>Crucial: Ensure <code>kustomize-controller</code> service account has permissions to manage resources in target namespaces (e.g., <code>home-automation</code>, <code>media</code>) from the source namespace <code>flux-apps</code>. This is usually handled by ClusterRoles, but explicit RoleBindings may be needed if strict isolation is used.</li> </ol> </li> </ul>"},{"location":"standardization/todo-refactor/#24-flux-operator-strategy","title":"2.4. \"Flux Operator\" Strategy","text":"<p>Decision: Stick with the standard GitOps Toolkit (GOTK) components (<code>source-controller</code>, <code>kustomize-controller</code>, etc.) rather than switching to the \"Flux Operator\" (control-plane-less CRD manager). *   Reasoning: <code>onedr0p</code> uses standard GOTK components. The \"Flux Operator\" is a different architectural pattern often used for managing Flux itself as a resource. Adopting <code>onedr0p</code>'s structure (OCI, Global Patches, Components) achieves the desired modernization without re-architecting the control plane.</p>"},{"location":"standardization/todo-refactor/#3-monitoring-health-checks","title":"3. Monitoring &amp; Health Checks","text":"<p>Goal: Automate health check discovery using Gatus sidecars and HTTPRoutes.</p>"},{"location":"standardization/todo-refactor/#overview_2","title":"Overview","text":"<ul> <li>Gatus Sidecar: Use the sidecar pattern to automatically discover <code>HTTPRoute</code> resources.</li> <li>Probes: Ensure all <code>HelmReleases</code> define <code>liveness</code>, <code>readiness</code>, and <code>startup</code> probes.</li> <li>Alerting: Configure Alertmanager to route critical alerts to Pushover.</li> </ul>"},{"location":"standardization/todo-refactor/#detailed-implementation-plan_2","title":"Detailed Implementation Plan","text":""},{"location":"standardization/todo-refactor/#31-update-gatus-helmrelease","title":"3.1. Update Gatus HelmRelease","text":"<p>Enable the sidecar pattern used by <code>onedr0p</code>.</p> <ol> <li> <p>Modify <code>kubernetes/main/apps/observability/gatus/app/helmrelease.yaml</code>:     Add the sidecar container.     <pre><code>initContainers:\n  gatus-sidecar:\n    image:\n      repository: ghcr.io/home-operations/gatus-sidecar\n      tag: v0.0.11\n    args:\n      - --auto-httproute # Critical: watches HTTPRoutes\n</code></pre></p> </li> <li> <p>RBAC: Ensure the Gatus service account has permission to <code>list</code> and <code>watch</code> <code>HTTPRoutes</code> and <code>Gateways</code>.</p> </li> </ol>"},{"location":"standardization/todo-refactor/#32-annotate-apps","title":"3.2. Annotate Apps","text":"<p>Once using <code>HTTPRoute</code>, add the annotation to enable monitoring: <pre><code>route:\n  app:\n    annotations:\n      gatus.home-operations.com/enabled: \"true\"\n</code></pre></p>"},{"location":"standardization/todo-refactor/#5-templates-vs-components","title":"5. Templates vs. Components","text":"<p>Goal: Migrate from using Kustomize Templates (copied files) to Components (reusable overlays) to DRY (Don't Repeat Yourself) up the codebase.</p>"},{"location":"standardization/todo-refactor/#overview_3","title":"Overview","text":"<ul> <li>Current State: <code>haynes-ops</code> copies <code>ks.yaml</code> and other resource manifests into every app directory.</li> <li>Target State: Use Kustomize Components for shared patterns like VolSync replication, Gatus configuration, and common alerts.</li> <li>Why:<ul> <li>Updates to a pattern (e.g., changing VolSync schedule) only need to happen in one place (<code>kubernetes/shared/components</code>) instead of every single app.</li> <li>Cleaner app directories.</li> </ul> </li> </ul>"},{"location":"standardization/todo-refactor/#detailed-implementation-plan_3","title":"Detailed Implementation Plan","text":""},{"location":"standardization/todo-refactor/#51-create-shared-components-directory","title":"5.1. Create Shared Components Directory","text":"<p>Create a structure in <code>kubernetes/shared/components</code>: <pre><code>kubernetes/shared/components/\n\u251c\u2500\u2500 common/\n\u251c\u2500\u2500 gatus/\n\u2502   \u251c\u2500\u2500 guarded/\n\u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml\n\u2502   \u2502   \u2514\u2500\u2500 route-patch.yaml\n\u2502   \u2514\u2500\u2500 external/\n\u2502       \u251c\u2500\u2500 kustomization.yaml\n\u2502       \u2514\u2500\u2500 route-patch.yaml\n\u2514\u2500\u2500 volsync/\n    \u251c\u2500\u2500 r2/\n    \u2502   \u251c\u2500\u2500 kustomization.yaml\n    \u2502   \u251c\u2500\u2500 replication-source.yaml\n    \u2502   \u2514\u2500\u2500 replication-destination.yaml\n    \u2514\u2500\u2500 b2/ ...\n</code></pre></p>"},{"location":"standardization/todo-refactor/#52-define-a-component-example-gatus-guarded","title":"5.2. Define a Component (Example: Gatus Guarded)","text":"<p>In <code>kubernetes/shared/components/gatus/guarded/kustomization.yaml</code>: <pre><code>apiVersion: kustomize.config.k8s.io/v1alpha1\nkind: Component\npatches:\n  - path: route-patch.yaml\n    target:\n      kind: HTTPRoute\n</code></pre></p> <p>In <code>kubernetes/shared/components/gatus/guarded/route-patch.yaml</code>: <pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: not-used\n  annotations:\n    gatus.home-operations.com/enabled: \"true\"\n    gatus.home-operations.com/path: /\n</code></pre></p>"},{"location":"standardization/todo-refactor/#53-consume-component-in-app","title":"5.3. Consume Component in App","text":"<p>Update an app's <code>kustomization.yaml</code> (e.g., <code>plex</code>) to use the component instead of defining raw resources.</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - ./helmrelease.yaml\ncomponents:\n  - ../../../../../shared/components/gatus/guarded\n</code></pre>"},{"location":"standardization/todo-refactor/#6-taskfile-cleanup","title":"6. Taskfile Cleanup","text":"<p>Goal: Simplify local development workflow by removing complex, unused tasks.</p>"},{"location":"standardization/todo-refactor/#overview_4","title":"Overview","text":"<p><code>haynes-ops</code> Taskfiles are currently overly complex. We will strip them down to essential commands for <code>cluster</code>, <code>flux</code>, and <code>sops</code>.</p>"},{"location":"standardization/todo-refactor/#detailed-implementation-plan_4","title":"Detailed Implementation Plan","text":""},{"location":"standardization/todo-refactor/#41-simplify-taskfileyaml","title":"4.1. Simplify <code>Taskfile.yaml</code>","text":"<p>Remove unused includes. Keep it flat if possible.</p> <p>Proposed Structure: <pre><code>version: \"3\"\nincludes:\n  kubernetes: .taskfiles/Kubernetes/Taskfile.yaml\n  flux: .taskfiles/Flux/Taskfile.yaml\n  sops: .taskfiles/Sops/Taskfile.yaml\n\ntasks:\n  default: task -l\n</code></pre></p>"},{"location":"standardization/todo-refactor/#42-prune-flux-taskfile","title":"4.2. Prune Flux Taskfile","text":"<p>Reduce <code>Flux/Taskfile.yaml</code> to essentials: *   <code>verify</code>: <code>flux reconcile kustomization cluster ...</code> *   <code>sync</code>: <code>flux reconcile source git flux-system ...</code></p>"},{"location":"standardization/todo-refactor/#6-execution-order-risk-based-roadmap","title":"6. Execution Order (Risk-Based Roadmap)","text":"<ol> <li>Week 1: Housekeeping (Low Risk)<ul> <li>Clean Taskfiles.</li> <li>Migrate <code>HelmRepository</code> \u2192 <code>OCIRepository</code> (Low risk).</li> </ul> </li> <li>Week 2: Flux Structure &amp; Components (Medium Risk)<ul> <li>Implement Global Patches in <code>flux/apps.yaml</code>.</li> <li>Refactor <code>Templates</code> \u2192 <code>Components</code>.</li> <li>Simplify individual <code>ks.yaml</code> files.</li> </ul> </li> <li>Week 3: Network on Edge (High Risk)<ul> <li>Install Gateway API CRDs on <code>edge</code> cluster.</li> <li>Configure Traefik for Gateway API.</li> <li>Migrate one app to <code>HTTPRoute</code> on <code>edge</code>.</li> </ul> </li> <li>Week 4: Monitoring (Medium Risk)<ul> <li>Deploy Gatus Sidecar.</li> <li>Verify auto-discovery works with the migrated app.</li> </ul> </li> </ol>"},{"location":"storage/snapshot-upgrade/","title":"Snapshot upgrade","text":"<p>cephfilesystemsubvolumegroups.ceph.rook.io                 2024-10-31T20:43:22Z volumegroupsnapshotclasses.groupsnapshot.storage.k8s.io    2025-01-19T02:24:00Z volumegroupsnapshotcontents.groupsnapshot.storage.k8s.io   2024-10-31T20:43:01Z volumegroupsnapshots.groupsnapshot.storage.k8s.io          2024-10-31T20:43:01Z</p> <p>See this thread:</p> <pre><code>kubectl patch crd volumegroupsnapshots.groupsnapshot.storage.k8s.io --subresource='status' --type='merge' -p '{\"status\":{\"storedVersions\": [\"v1beta1\"]}}'\n</code></pre>"},{"location":"the-move/checklist/","title":"Checklist","text":""},{"location":"the-move/checklist/#router","title":"Router","text":"<ul> <li>[ ] Setup same subnets and VLANs</li> <li>[ ] Setup HNET+ WiFi for 5Ghz and 6Ghz </li> <li>[ ] Setup HNETIoT WiFi for 2.4Ghz and in the IoT VLAN. Only add 2.4Ghz - make sure the password matches (in esphome 1Password)</li> <li>[ ] Channel Width 20 for 2.4 Ghz, TBD if we use Auto for channel</li> <li>[ ] Channel Width 80 for 5Ghz &amp; Auto Channel</li> <li>[ ] Channel Width 160 for 6Ghz &amp; Auto Channel </li> <li>[ ] Sonos Hoops</li> <li>[ ] Host (A) DNS for tubeszb-zigbee01.haynesnetwork -&gt; 192.168.50.162</li> <li>[ ] Host (A) DNS for tubeszb-zwave01.haynesnetwork -&gt; 192.168.50.92</li> <li>[ ] Host (A) DNS for internal.haynesops -&gt; 192.168.40.203</li> <li>[ ] Setup external for cloudflare and forward ports etc (TODO document this)</li> </ul> <p>TODO Review the rest of the DNS stuff</p> <ul> <li>[ ] Sort out Talos01-03 IPs</li> </ul>"},{"location":"the-move/checklist/#appendix","title":"Appendix","text":""},{"location":"the-move/checklist/#vlans","title":"VLANs","text":"<p>| VLAN # | Subnet          | Name        | Decription                                   |  | 1      | 192.168.0.0/24  | Default     |                                              | | 2      | 192.168.20.0/24 | CephLan     | Isolated, no internet, for proxmox ceph only | | 3      | 192.168.30.0/24 | VPNLan      | Bound to https://mullvad.net/en              | | 4      | 192.168.40.0/24 | Hayneslab   | Configued wrt k8s loadbalacer pools          | | 5      | 192.168.50.0/24 | IoT         | TODO needs work to be better isolate         |  | 6      | 192.168.60.0/24 | RookLan     | Isolated, no internet, for rook-ceph only    |</p>"},{"location":"the-move/checklist/#dns-entries","title":"DNS Entries","text":""},{"location":"the-move/checklist/#hayneslab-vlam","title":"HaynesLab VLAM","text":"Record Fixed IP haynesintelligence.haynesnetwork 192.168.40.11 nas01.haynesnetwork 192.168.40.52 nut02.haynesnetwork 192.168.40.53 talosm01.haynesnetwork 192.168.40.93 talosm02.haynesnetwork 192.168.40.59 talosm03.haynesnetwork 192.168.40.10 pikvm.haynesnetwork 192.168.40.66 N/A (TESmart KVM) 192.168.40.70"},{"location":"the-move/checklist/#iot-vlan","title":"IoT VLAN","text":"Record Fixed IP tubeszb-zigbee01.haynesnetwork 192.168.50.162 tubeszb-zwave01.haynesnetwork 192.168.50.92"},{"location":"this-site/","title":"How This Site","text":"<p>This site is build using Material for MkDocs</p>"},{"location":"this-site/#cool-stuff","title":"Cool Stuff","text":"<p>References provides a guide to how to use some cool stuff. </p> <p>Admonitions are particularly interesting.</p>"}]}